{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "mount_file_id": "1xW75M4TsDoO6xAyikC5jX2npjDThEP7B",
   "authorship_tag": "ABX9TyNEtAJPUVCOv7S2LZWWOjkz"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Baseline model\n",
    "hidden_layer_sizes=(256, 128), <br> activation='relu', <br> solver='adam', <br> alpha=1e-4, <br> batch_size=128, <br> learning_rate='adaptive', <br> learning_rate_init=1e-3, <br> max_iter=400, <br> early_stopping=True"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EMYCvIJijpC3",
    "ExecuteTime": {
     "end_time": "2025-11-28T02:55:51.316548Z",
     "start_time": "2025-11-28T02:55:50.584736Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_recall_curve, auc, classification_report"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv('../../../dataset/open_world/openworld_train.csv')\n",
    "test_df = pd.read_csv('../../../dataset/open_world/openworld_test.csv')"
   ],
   "metadata": {
    "id": "8dYQpDerj9IR",
    "ExecuteTime": {
     "end_time": "2025-11-28T02:55:51.394031Z",
     "start_time": "2025-11-28T02:55:51.320200Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# Feature / Target Split\n",
    "X = train_df.drop(columns=[\"label\"]).values\n",
    "y = train_df[\"label\"].values\n",
    "\n",
    "X_test = test_df.drop(columns=[\"label\"]).values\n",
    "y_test = test_df[\"label\"].values\n",
    "\n",
    "# Train / Validation Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ],
   "metadata": {
    "id": "RqSYCaZ2kpKw",
    "ExecuteTime": {
     "end_time": "2025-11-28T02:55:51.406135Z",
     "start_time": "2025-11-28T02:55:51.398300Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(256, 128),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=1e-4,\n",
    "    batch_size=128,\n",
    "    learning_rate='adaptive',\n",
    "    learning_rate_init=1e-3,\n",
    "    max_iter=400,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.15,\n",
    "    n_iter_no_change=15,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "id": "e1M13GaTkpic",
    "ExecuteTime": {
     "end_time": "2025-11-28T02:55:51.410785Z",
     "start_time": "2025-11-28T02:55:51.408984Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\n[INFO] Training MLP (256-128) on Open-world dataset...\")\n",
    "mlp.fit(X_train, y_train)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Fj0qN-z_lnJ7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762976604057,
     "user_tz": -540,
     "elapsed": 61921,
     "user": {
      "displayName": "­신은서(엘텍공과대학 소프트웨어학부)",
      "userId": "09254021355381794528"
     }
    },
    "outputId": "c1b1723d-d129-475c-a44c-c854a30b6a72",
    "ExecuteTime": {
     "end_time": "2025-11-28T02:56:16.945919Z",
     "start_time": "2025-11-28T02:55:51.414874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Training MLP (256-128) on Open-world dataset...\n",
      "Iteration 1, loss = 3.39306217\n",
      "Validation score: 0.364122\n",
      "Iteration 2, loss = 2.62049841\n",
      "Validation score: 0.426108\n",
      "Iteration 3, loss = 2.28847995\n",
      "Validation score: 0.460591\n",
      "Iteration 4, loss = 2.11910486\n",
      "Validation score: 0.472496\n",
      "Iteration 5, loss = 1.99083459\n",
      "Validation score: 0.498358\n",
      "Iteration 6, loss = 1.88876947\n",
      "Validation score: 0.504516\n",
      "Iteration 7, loss = 1.80398505\n",
      "Validation score: 0.524631\n",
      "Iteration 8, loss = 1.72404954\n",
      "Validation score: 0.527504\n",
      "Iteration 9, loss = 1.66075334\n",
      "Validation score: 0.543103\n",
      "Iteration 10, loss = 1.60159090\n",
      "Validation score: 0.550082\n",
      "Iteration 11, loss = 1.54422921\n",
      "Validation score: 0.559524\n",
      "Iteration 12, loss = 1.49226115\n",
      "Validation score: 0.575123\n",
      "Iteration 13, loss = 1.44778216\n",
      "Validation score: 0.569376\n",
      "Iteration 14, loss = 1.40193709\n",
      "Validation score: 0.581691\n",
      "Iteration 15, loss = 1.36310495\n",
      "Validation score: 0.591133\n",
      "Iteration 16, loss = 1.33300444\n",
      "Validation score: 0.600985\n",
      "Iteration 17, loss = 1.29414896\n",
      "Validation score: 0.602217\n",
      "Iteration 18, loss = 1.26129713\n",
      "Validation score: 0.607964\n",
      "Iteration 19, loss = 1.23437751\n",
      "Validation score: 0.610837\n",
      "Iteration 20, loss = 1.20652128\n",
      "Validation score: 0.602217\n",
      "Iteration 21, loss = 1.18160715\n",
      "Validation score: 0.605501\n",
      "Iteration 22, loss = 1.14758568\n",
      "Validation score: 0.607964\n",
      "Iteration 23, loss = 1.12724895\n",
      "Validation score: 0.619048\n",
      "Iteration 24, loss = 1.10406189\n",
      "Validation score: 0.618637\n",
      "Iteration 25, loss = 1.08157563\n",
      "Validation score: 0.623563\n",
      "Iteration 26, loss = 1.05874612\n",
      "Validation score: 0.625616\n",
      "Iteration 27, loss = 1.03926382\n",
      "Validation score: 0.632184\n",
      "Iteration 28, loss = 1.01867406\n",
      "Validation score: 0.638752\n",
      "Iteration 29, loss = 0.99910332\n",
      "Validation score: 0.630131\n",
      "Iteration 30, loss = 0.97788099\n",
      "Validation score: 0.620690\n",
      "Iteration 31, loss = 0.96446587\n",
      "Validation score: 0.636289\n",
      "Iteration 32, loss = 0.95470967\n",
      "Validation score: 0.637931\n",
      "Iteration 33, loss = 0.93266692\n",
      "Validation score: 0.637110\n",
      "Iteration 34, loss = 0.91545668\n",
      "Validation score: 0.635057\n",
      "Iteration 35, loss = 0.90635092\n",
      "Validation score: 0.633826\n",
      "Iteration 36, loss = 0.88367473\n",
      "Validation score: 0.633826\n",
      "Iteration 37, loss = 0.87121267\n",
      "Validation score: 0.646552\n",
      "Iteration 38, loss = 0.85500982\n",
      "Validation score: 0.648604\n",
      "Iteration 39, loss = 0.84139205\n",
      "Validation score: 0.635878\n",
      "Iteration 40, loss = 0.83258847\n",
      "Validation score: 0.646962\n",
      "Iteration 41, loss = 0.81548952\n",
      "Validation score: 0.649425\n",
      "Iteration 42, loss = 0.79814305\n",
      "Validation score: 0.639984\n",
      "Iteration 43, loss = 0.79093838\n",
      "Validation score: 0.639984\n",
      "Iteration 44, loss = 0.77938372\n",
      "Validation score: 0.653530\n",
      "Iteration 45, loss = 0.76477682\n",
      "Validation score: 0.646962\n",
      "Iteration 46, loss = 0.75337763\n",
      "Validation score: 0.655172\n",
      "Iteration 47, loss = 0.74149359\n",
      "Validation score: 0.648604\n",
      "Iteration 48, loss = 0.72819541\n",
      "Validation score: 0.639984\n",
      "Iteration 49, loss = 0.72687603\n",
      "Validation score: 0.636700\n",
      "Iteration 50, loss = 0.71168474\n",
      "Validation score: 0.648604\n",
      "Iteration 51, loss = 0.70150824\n",
      "Validation score: 0.650246\n",
      "Iteration 52, loss = 0.68622043\n",
      "Validation score: 0.653530\n",
      "Iteration 53, loss = 0.68256441\n",
      "Validation score: 0.652709\n",
      "Iteration 54, loss = 0.67384054\n",
      "Validation score: 0.655172\n",
      "Iteration 55, loss = 0.66749503\n",
      "Validation score: 0.659688\n",
      "Iteration 56, loss = 0.65497891\n",
      "Validation score: 0.664614\n",
      "Iteration 57, loss = 0.64401078\n",
      "Validation score: 0.635878\n",
      "Iteration 58, loss = 0.62773676\n",
      "Validation score: 0.653941\n",
      "Iteration 59, loss = 0.62736955\n",
      "Validation score: 0.662972\n",
      "Iteration 60, loss = 0.62003001\n",
      "Validation score: 0.665025\n",
      "Iteration 61, loss = 0.61049712\n",
      "Validation score: 0.651888\n",
      "Iteration 62, loss = 0.60177678\n",
      "Validation score: 0.650246\n",
      "Iteration 63, loss = 0.58965663\n",
      "Validation score: 0.666667\n",
      "Iteration 64, loss = 0.58266406\n",
      "Validation score: 0.669540\n",
      "Iteration 65, loss = 0.58070811\n",
      "Validation score: 0.653941\n",
      "Iteration 66, loss = 0.57635740\n",
      "Validation score: 0.654351\n",
      "Iteration 67, loss = 0.55984912\n",
      "Validation score: 0.662562\n",
      "Iteration 68, loss = 0.55528567\n",
      "Validation score: 0.668719\n",
      "Iteration 69, loss = 0.54492048\n",
      "Validation score: 0.665846\n",
      "Iteration 70, loss = 0.53921719\n",
      "Validation score: 0.662562\n",
      "Iteration 71, loss = 0.53092746\n",
      "Validation score: 0.671593\n",
      "Iteration 72, loss = 0.52594883\n",
      "Validation score: 0.660509\n",
      "Iteration 73, loss = 0.52363322\n",
      "Validation score: 0.659278\n",
      "Iteration 74, loss = 0.50866936\n",
      "Validation score: 0.658456\n",
      "Iteration 75, loss = 0.50685114\n",
      "Validation score: 0.672824\n",
      "Iteration 76, loss = 0.49514051\n",
      "Validation score: 0.663793\n",
      "Iteration 77, loss = 0.49299286\n",
      "Validation score: 0.660509\n",
      "Iteration 78, loss = 0.48121028\n",
      "Validation score: 0.662562\n",
      "Iteration 79, loss = 0.47920364\n",
      "Validation score: 0.672003\n",
      "Iteration 80, loss = 0.47880510\n",
      "Validation score: 0.660920\n",
      "Iteration 81, loss = 0.46960595\n",
      "Validation score: 0.650246\n",
      "Iteration 82, loss = 0.46593304\n",
      "Validation score: 0.665846\n",
      "Iteration 83, loss = 0.45050407\n",
      "Validation score: 0.667488\n",
      "Iteration 84, loss = 0.44701770\n",
      "Validation score: 0.659688\n",
      "Iteration 85, loss = 0.44537198\n",
      "Validation score: 0.664614\n",
      "Iteration 86, loss = 0.43905818\n",
      "Validation score: 0.657225\n",
      "Iteration 87, loss = 0.42983247\n",
      "Validation score: 0.669951\n",
      "Iteration 88, loss = 0.42479535\n",
      "Validation score: 0.658456\n",
      "Iteration 89, loss = 0.41556934\n",
      "Validation score: 0.661330\n",
      "Iteration 90, loss = 0.41436913\n",
      "Validation score: 0.666256\n",
      "Iteration 91, loss = 0.41066826\n",
      "Validation score: 0.660509\n",
      "Validation score did not improve more than tol=0.000100 for 15 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(batch_size=128, early_stopping=True,\n",
       "              hidden_layer_sizes=(256, 128), learning_rate='adaptive',\n",
       "              max_iter=400, n_iter_no_change=15, random_state=42,\n",
       "              validation_fraction=0.15, verbose=True)"
      ],
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(batch_size=128, early_stopping=True,\n",
       "              hidden_layer_sizes=(256, 128), learning_rate=&#x27;adaptive&#x27;,\n",
       "              max_iter=400, n_iter_no_change=15, random_state=42,\n",
       "              validation_fraction=0.15, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(batch_size=128, early_stopping=True,\n",
       "              hidden_layer_sizes=(256, 128), learning_rate=&#x27;adaptive&#x27;,\n",
       "              max_iter=400, n_iter_no_change=15, random_state=42,\n",
       "              validation_fraction=0.15, verbose=True)</pre></div></div></div></div></div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "# Val\n",
    "val_pred = mlp.predict(X_val)\n",
    "val_acc = accuracy_score(y_val, val_pred)\n",
    "val_f1_macro = f1_score(y_val, val_pred, average='macro')\n",
    "val_f1_weighted = f1_score(y_val, val_pred, average='weighted')\n",
    "\n",
    "print(\"\\n[VAL RESULTS]\")\n",
    "print(f\"Accuracy     : {val_acc:.4f}\")\n",
    "print(f\"F1 (macro)   : {val_f1_macro:.4f}\")\n",
    "print(f\"F1 (weighted): {val_f1_weighted:.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qyxiqFMZlqpF",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762976626529,
     "user_tz": -540,
     "elapsed": 426,
     "user": {
      "displayName": "­신은서(엘텍공과대학 소프트웨어학부)",
      "userId": "09254021355381794528"
     }
    },
    "outputId": "d69eef95-995f-44a3-927e-f4f4df0354e1",
    "ExecuteTime": {
     "end_time": "2025-11-28T02:56:16.980782Z",
     "start_time": "2025-11-28T02:56:16.951386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[VAL RESULTS]\n",
      "Accuracy     : 0.6608\n",
      "F1 (macro)   : 0.5788\n",
      "F1 (weighted): 0.6560\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "# Test prediction\n",
    "test_pred = mlp.predict(X_test)\n",
    "test_proba = mlp.predict_proba(X_test)\n",
    "\n",
    "# Accuracy\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "\n",
    "# F1 scores\n",
    "test_f1_macro = f1_score(y_test, test_pred, average='macro')\n",
    "test_f1_micro = f1_score(y_test, test_pred, average='micro')\n",
    "test_f1_weighted = f1_score(y_test, test_pred, average='weighted')\n",
    "\n",
    "# ROC-AUC (macro)\n",
    "test_roc_auc = roc_auc_score(y_test, test_proba, multi_class='ovr', average='macro')\n",
    "\n",
    "# PR-AUC (macro)\n",
    "pr_aucs = []\n",
    "for c in range(test_proba.shape[1]):\n",
    "    y_true_binary = (y_test == c).astype(int)\n",
    "    prec, rec, _ = precision_recall_curve(y_true_binary, test_proba[:, c])\n",
    "    pr_aucs.append(auc(rec, prec))\n",
    "test_pr_auc_macro = np.mean(pr_aucs)\n",
    "\n",
    "# PRINT\n",
    "print(\"\\n========== [TEST RESULTS] ==========\")\n",
    "print(f\"Accuracy        : {test_acc:.4f}\")\n",
    "print(f\"F1 (macro)      : {test_f1_macro:.4f}\")\n",
    "print(f\"F1 (micro)      : {test_f1_micro:.4f}\")\n",
    "print(f\"F1 (weighted)   : {test_f1_weighted:.4f}\")\n",
    "print(f\"ROC-AUC (macro) : {test_roc_auc:.4f}\")\n",
    "print(f\"PR-AUC (macro)  : {test_pr_auc_macro:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report :\")\n",
    "print(classification_report(y_test, test_pred, digits=4))"
   ],
   "metadata": {
    "id": "YS4StQ5fmIAP",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762976631060,
     "user_tz": -540,
     "elapsed": 273,
     "user": {
      "displayName": "­신은서(엘텍공과대학 소프트웨어학부)",
      "userId": "09254021355381794528"
     }
    },
    "outputId": "eec33c7c-8edc-4353-aed9-af49b120ea33",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2025-11-28T02:56:17.293003Z",
     "start_time": "2025-11-28T02:56:16.990875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== [TEST RESULTS] ==========\n",
      "Accuracy        : 0.6697\n",
      "F1 (macro)      : 0.5930\n",
      "F1 (micro)      : 0.6697\n",
      "F1 (weighted)   : 0.6656\n",
      "ROC-AUC (macro) : 0.9779\n",
      "PR-AUC (macro)  : 0.6192\n",
      "\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6415    0.5667    0.6018        60\n",
      "           1     0.6769    0.7333    0.7040        60\n",
      "           2     0.5294    0.6000    0.5625        60\n",
      "           3     0.5455    0.4000    0.4615        60\n",
      "           4     0.7209    0.5167    0.6019        60\n",
      "           5     0.4118    0.3500    0.3784        60\n",
      "           6     0.6338    0.7500    0.6870        60\n",
      "           7     0.6750    0.4500    0.5400        60\n",
      "           8     0.4096    0.5667    0.4755        60\n",
      "           9     0.6216    0.3833    0.4742        60\n",
      "          10     0.6000    0.5000    0.5455        60\n",
      "          11     0.7073    0.4833    0.5743        60\n",
      "          12     0.8000    0.6000    0.6857        60\n",
      "          13     0.2593    0.2333    0.2456        60\n",
      "          14     0.7073    0.4833    0.5743        60\n",
      "          15     0.4474    0.5667    0.5000        60\n",
      "          16     0.7955    0.5833    0.6731        60\n",
      "          17     0.3971    0.4500    0.4219        60\n",
      "          18     0.7273    0.5333    0.6154        60\n",
      "          19     0.5000    0.7000    0.5833        60\n",
      "          20     0.7941    0.9000    0.8438        60\n",
      "          21     0.6190    0.4333    0.5098        60\n",
      "          22     0.4590    0.4667    0.4628        60\n",
      "          23     0.3909    0.7167    0.5059        60\n",
      "          24     0.4462    0.4833    0.4640        60\n",
      "          25     0.6000    0.3500    0.4421        60\n",
      "          26     0.5152    0.8500    0.6415        60\n",
      "          27     0.3902    0.5333    0.4507        60\n",
      "          28     0.5574    0.5667    0.5620        60\n",
      "          29     0.8103    0.7833    0.7966        60\n",
      "          30     0.7321    0.6833    0.7069        60\n",
      "          31     0.6327    0.5167    0.5688        60\n",
      "          32     0.5968    0.6167    0.6066        60\n",
      "          33     0.6081    0.7500    0.6716        60\n",
      "          34     0.5652    0.4333    0.4906        60\n",
      "          35     0.6271    0.6167    0.6218        60\n",
      "          36     0.5867    0.7333    0.6519        60\n",
      "          37     0.3571    0.4167    0.3846        60\n",
      "          38     0.5614    0.5333    0.5470        60\n",
      "          39     0.6780    0.6667    0.6723        60\n",
      "          40     0.6667    0.3333    0.4444        60\n",
      "          41     0.6267    0.7833    0.6963        60\n",
      "          42     0.5000    0.5333    0.5161        60\n",
      "          43     0.7857    0.5500    0.6471        60\n",
      "          44     0.9298    0.8833    0.9060        60\n",
      "          45     0.5000    0.2000    0.2857        60\n",
      "          46     0.7222    0.4333    0.5417        60\n",
      "          47     0.6279    0.4500    0.5243        60\n",
      "          48     0.5238    0.3667    0.4314        60\n",
      "          49     0.7297    0.4500    0.5567        60\n",
      "          50     0.7292    0.5833    0.6481        60\n",
      "          51     0.5781    0.6167    0.5968        60\n",
      "          52     0.5238    0.5500    0.5366        60\n",
      "          53     0.7500    0.8000    0.7742        60\n",
      "          54     0.7209    0.5167    0.6019        60\n",
      "          55     0.4795    0.5833    0.5263        60\n",
      "          56     0.7833    0.7833    0.7833        60\n",
      "          57     0.7049    0.7167    0.7107        60\n",
      "          58     0.6901    0.8167    0.7481        60\n",
      "          59     0.9375    0.7500    0.8333        60\n",
      "          60     0.5167    0.5167    0.5167        60\n",
      "          61     0.6296    0.5667    0.5965        60\n",
      "          62     0.5789    0.5500    0.5641        60\n",
      "          63     0.5342    0.6500    0.5865        60\n",
      "          64     0.6889    0.5167    0.5905        60\n",
      "          65     0.6429    0.4500    0.5294        60\n",
      "          66     0.6220    0.8500    0.7183        60\n",
      "          67     0.7015    0.7833    0.7402        60\n",
      "          68     0.4795    0.5833    0.5263        60\n",
      "          69     0.7368    0.7000    0.7179        60\n",
      "          70     0.6625    0.8833    0.7571        60\n",
      "          71     0.5000    0.5833    0.5385        60\n",
      "          72     0.4898    0.4000    0.4404        60\n",
      "          73     0.8519    0.7667    0.8070        60\n",
      "          74     0.4928    0.5667    0.5271        60\n",
      "          75     0.8070    0.7667    0.7863        60\n",
      "          76     0.8305    0.8167    0.8235        60\n",
      "          77     0.3333    0.3000    0.3158        60\n",
      "          78     0.5556    0.3333    0.4167        60\n",
      "          79     0.3636    0.4667    0.4088        60\n",
      "          80     0.7321    0.6833    0.7069        60\n",
      "          81     0.5634    0.6667    0.6107        60\n",
      "          82     0.7347    0.6000    0.6606        60\n",
      "          83     0.4062    0.4333    0.4194        60\n",
      "          84     0.8776    0.7167    0.7890        60\n",
      "          85     0.7925    0.7000    0.7434        60\n",
      "          86     0.8000    0.8667    0.8320        60\n",
      "          87     0.7241    0.7000    0.7119        60\n",
      "          88     0.6154    0.4000    0.4848        60\n",
      "          89     0.3651    0.3833    0.3740        60\n",
      "          90     0.6377    0.7333    0.6822        60\n",
      "          91     0.5849    0.5167    0.5487        60\n",
      "          92     0.3654    0.3167    0.3393        60\n",
      "          93     0.8358    0.9333    0.8819        60\n",
      "          94     0.5467    0.6833    0.6074        60\n",
      "          95     0.7865    0.8303    0.8078      3000\n",
      "\n",
      "    accuracy                         0.6697      8700\n",
      "   macro avg     0.6170    0.5876    0.5930      8700\n",
      "weighted avg     0.6743    0.6697    0.6656      8700\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ]
}
