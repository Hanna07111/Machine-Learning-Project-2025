{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "mount_file_id": "11FZLjd4kfeX81su7EPgrUESrgrAiuaU-",
   "authorship_tag": "ABX9TyPvhrMK8rb+jfQk328Z5vqj"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Baseline model\n",
    "hidden_layer_sizes=(256, 128),   # 2층 MLP <br> activation='relu', <br> solver='adam', <br> alpha=1e-4,         # L2 regularization <br> batch_size=128, <br> learning_rate='adaptive', <br> learning_rate_init=1e-3, <br> max_iter=200, <br> early_stopping=True"
   ],
   "metadata": {
    "id": "fsC7F6enJNdi"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Xsxo0QnM4-ii",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762876802589,
     "user_tz": -540,
     "elapsed": 2590,
     "user": {
      "displayName": "­신은서(엘텍공과대학 소프트웨어학부)",
      "userId": "09254021355381794528"
     }
    },
    "ExecuteTime": {
     "end_time": "2025-11-28T02:37:01.830388Z",
     "start_time": "2025-11-28T02:37:00.347377Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv('../../dataset/closed_world/closedworld_train.csv')\n",
    "test_df  = pd.read_csv('../../dataset/closed_world/closedworld_test.csv')\n",
    "\n",
    "print(\"raw train shape :\", train_df.shape)\n",
    "print(\"raw test shape  :\", test_df.shape)\n",
    "print(\"raw unique labels in train:\", train_df[\"label\"].nunique())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EsiLDHvq5wXM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762876824441,
     "user_tz": -540,
     "elapsed": 1776,
     "user": {
      "displayName": "­신은서(엘텍공과대학 소프트웨어학부)",
      "userId": "09254021355381794528"
     }
    },
    "outputId": "6bb05785-6877-40e4-c837-5f61b2893757",
    "ExecuteTime": {
     "end_time": "2025-11-28T02:37:01.907750Z",
     "start_time": "2025-11-28T02:37:01.844695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw train shape : (13300, 27)\n",
      "raw test shape  : (5700, 27)\n",
      "raw unique labels in train: 95\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# feature/target 분리\n",
    "X = train_df.drop(columns=[\"label\"]).values\n",
    "y = train_df[\"label\"].values\n",
    "\n",
    "X_test = test_df.drop(columns=[\"label\"]).values\n",
    "y_test = test_df[\"label\"].values\n",
    "\n",
    "# train/val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"[INFO] Train set: {X_train.shape}, Val set: {X_val.shape}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9JXODwIT6MfI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762876826603,
     "user_tz": -540,
     "elapsed": 25,
     "user": {
      "displayName": "­신은서(엘텍공과대학 소프트웨어학부)",
      "userId": "09254021355381794528"
     }
    },
    "outputId": "d1ce4486-8074-417b-c2ef-db824c5f9eb3",
    "ExecuteTime": {
     "end_time": "2025-11-28T02:37:01.922104Z",
     "start_time": "2025-11-28T02:37:01.913184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Train set: (10640, 26), Val set: (2660, 26)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(256, 128),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=1e-4,                           # L2 regularization\n",
    "    batch_size=128,\n",
    "    learning_rate='adaptive',\n",
    "    learning_rate_init=1e-3,\n",
    "    max_iter=400,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.15,\n",
    "    n_iter_no_change=15,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "id": "rc5S0I5G7M0H",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762876943420,
     "user_tz": -540,
     "elapsed": 10,
     "user": {
      "displayName": "­신은서(엘텍공과대학 소프트웨어학부)",
      "userId": "09254021355381794528"
     }
    },
    "ExecuteTime": {
     "end_time": "2025-11-28T02:37:01.949255Z",
     "start_time": "2025-11-28T02:37:01.946847Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\n[INFO] Training MLP (256-128) ...\")\n",
    "mlp.fit(X_train, y_train)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7NCM-tRJ8n2C",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762877022355,
     "user_tz": -540,
     "elapsed": 59041,
     "user": {
      "displayName": "­신은서(엘텍공과대학 소프트웨어학부)",
      "userId": "09254021355381794528"
     }
    },
    "outputId": "fe79edcc-07f0-4fae-d08b-e9f8b3f62b16",
    "ExecuteTime": {
     "end_time": "2025-11-28T02:37:30.088306Z",
     "start_time": "2025-11-28T02:37:01.973461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Training MLP (256-128) ...\n",
      "Iteration 1, loss = 4.19737907\n",
      "Validation score: 0.175439\n",
      "Iteration 2, loss = 3.21673406\n",
      "Validation score: 0.263784\n",
      "Iteration 3, loss = 2.78152775\n",
      "Validation score: 0.315789\n",
      "Iteration 4, loss = 2.56233111\n",
      "Validation score: 0.339599\n",
      "Iteration 5, loss = 2.39435241\n",
      "Validation score: 0.370927\n",
      "Iteration 6, loss = 2.26532798\n",
      "Validation score: 0.387845\n",
      "Iteration 7, loss = 2.15262626\n",
      "Validation score: 0.397870\n",
      "Iteration 8, loss = 2.05643832\n",
      "Validation score: 0.427945\n",
      "Iteration 9, loss = 1.97398602\n",
      "Validation score: 0.426065\n",
      "Iteration 10, loss = 1.90398910\n",
      "Validation score: 0.436717\n",
      "Iteration 11, loss = 1.82955477\n",
      "Validation score: 0.450501\n",
      "Iteration 12, loss = 1.76831408\n",
      "Validation score: 0.452381\n",
      "Iteration 13, loss = 1.71236957\n",
      "Validation score: 0.471805\n",
      "Iteration 14, loss = 1.65770675\n",
      "Validation score: 0.476817\n",
      "Iteration 15, loss = 1.60529269\n",
      "Validation score: 0.467419\n",
      "Iteration 16, loss = 1.56055179\n",
      "Validation score: 0.484336\n",
      "Iteration 17, loss = 1.52317828\n",
      "Validation score: 0.498747\n",
      "Iteration 18, loss = 1.48481724\n",
      "Validation score: 0.517544\n",
      "Iteration 19, loss = 1.44290409\n",
      "Validation score: 0.508772\n",
      "Iteration 20, loss = 1.40610446\n",
      "Validation score: 0.520677\n",
      "Iteration 21, loss = 1.37312268\n",
      "Validation score: 0.518797\n",
      "Iteration 22, loss = 1.34304681\n",
      "Validation score: 0.530702\n",
      "Iteration 23, loss = 1.30476268\n",
      "Validation score: 0.524436\n",
      "Iteration 24, loss = 1.27346070\n",
      "Validation score: 0.542607\n",
      "Iteration 25, loss = 1.23934383\n",
      "Validation score: 0.544486\n",
      "Iteration 26, loss = 1.22089198\n",
      "Validation score: 0.538221\n",
      "Iteration 27, loss = 1.19452435\n",
      "Validation score: 0.543233\n",
      "Iteration 28, loss = 1.16956481\n",
      "Validation score: 0.550125\n",
      "Iteration 29, loss = 1.14242397\n",
      "Validation score: 0.553258\n",
      "Iteration 30, loss = 1.11824272\n",
      "Validation score: 0.550125\n",
      "Iteration 31, loss = 1.09621426\n",
      "Validation score: 0.562657\n",
      "Iteration 32, loss = 1.07165867\n",
      "Validation score: 0.558271\n",
      "Iteration 33, loss = 1.04822813\n",
      "Validation score: 0.568296\n",
      "Iteration 34, loss = 1.03425464\n",
      "Validation score: 0.568296\n",
      "Iteration 35, loss = 1.00898426\n",
      "Validation score: 0.568922\n",
      "Iteration 36, loss = 0.99095114\n",
      "Validation score: 0.566416\n",
      "Iteration 37, loss = 0.96997778\n",
      "Validation score: 0.580827\n",
      "Iteration 38, loss = 0.95374564\n",
      "Validation score: 0.578947\n",
      "Iteration 39, loss = 0.93209247\n",
      "Validation score: 0.577068\n",
      "Iteration 40, loss = 0.90998611\n",
      "Validation score: 0.586466\n",
      "Iteration 41, loss = 0.88922829\n",
      "Validation score: 0.580201\n",
      "Iteration 42, loss = 0.88084973\n",
      "Validation score: 0.576441\n",
      "Iteration 43, loss = 0.86476830\n",
      "Validation score: 0.584586\n",
      "Iteration 44, loss = 0.84667895\n",
      "Validation score: 0.582707\n",
      "Iteration 45, loss = 0.83336304\n",
      "Validation score: 0.584586\n",
      "Iteration 46, loss = 0.81579835\n",
      "Validation score: 0.586466\n",
      "Iteration 47, loss = 0.80745184\n",
      "Validation score: 0.589599\n",
      "Iteration 48, loss = 0.79455614\n",
      "Validation score: 0.597118\n",
      "Iteration 49, loss = 0.76959551\n",
      "Validation score: 0.592105\n",
      "Iteration 50, loss = 0.76558973\n",
      "Validation score: 0.596491\n",
      "Iteration 51, loss = 0.74570470\n",
      "Validation score: 0.602130\n",
      "Iteration 52, loss = 0.73441069\n",
      "Validation score: 0.587093\n",
      "Iteration 53, loss = 0.72813666\n",
      "Validation score: 0.587093\n",
      "Iteration 54, loss = 0.70948090\n",
      "Validation score: 0.598997\n",
      "Iteration 55, loss = 0.69632103\n",
      "Validation score: 0.597118\n",
      "Iteration 56, loss = 0.68512126\n",
      "Validation score: 0.598371\n",
      "Iteration 57, loss = 0.66926733\n",
      "Validation score: 0.597118\n",
      "Iteration 58, loss = 0.65275948\n",
      "Validation score: 0.614035\n",
      "Iteration 59, loss = 0.65225779\n",
      "Validation score: 0.607769\n",
      "Iteration 60, loss = 0.63111644\n",
      "Validation score: 0.605263\n",
      "Iteration 61, loss = 0.62758428\n",
      "Validation score: 0.602130\n",
      "Iteration 62, loss = 0.60738391\n",
      "Validation score: 0.605263\n",
      "Iteration 63, loss = 0.60717520\n",
      "Validation score: 0.611529\n",
      "Iteration 64, loss = 0.59752509\n",
      "Validation score: 0.605890\n",
      "Iteration 65, loss = 0.58608133\n",
      "Validation score: 0.598371\n",
      "Iteration 66, loss = 0.56848904\n",
      "Validation score: 0.607143\n",
      "Iteration 67, loss = 0.55909895\n",
      "Validation score: 0.603383\n",
      "Iteration 68, loss = 0.55993094\n",
      "Validation score: 0.617168\n",
      "Iteration 69, loss = 0.53990996\n",
      "Validation score: 0.610276\n",
      "Iteration 70, loss = 0.53570212\n",
      "Validation score: 0.606516\n",
      "Iteration 71, loss = 0.52988984\n",
      "Validation score: 0.606516\n",
      "Iteration 72, loss = 0.51792413\n",
      "Validation score: 0.606516\n",
      "Iteration 73, loss = 0.51345537\n",
      "Validation score: 0.621554\n",
      "Iteration 74, loss = 0.49853598\n",
      "Validation score: 0.599624\n",
      "Iteration 75, loss = 0.48944681\n",
      "Validation score: 0.607143\n",
      "Iteration 76, loss = 0.48063136\n",
      "Validation score: 0.621554\n",
      "Iteration 77, loss = 0.47417294\n",
      "Validation score: 0.609023\n",
      "Iteration 78, loss = 0.46462603\n",
      "Validation score: 0.616541\n",
      "Iteration 79, loss = 0.45506958\n",
      "Validation score: 0.614035\n",
      "Iteration 80, loss = 0.45086569\n",
      "Validation score: 0.599624\n",
      "Iteration 81, loss = 0.44310396\n",
      "Validation score: 0.614662\n",
      "Iteration 82, loss = 0.43377402\n",
      "Validation score: 0.610276\n",
      "Iteration 83, loss = 0.42946232\n",
      "Validation score: 0.618421\n",
      "Iteration 84, loss = 0.41724718\n",
      "Validation score: 0.615915\n",
      "Iteration 85, loss = 0.41327407\n",
      "Validation score: 0.620301\n",
      "Iteration 86, loss = 0.40110803\n",
      "Validation score: 0.624060\n",
      "Iteration 87, loss = 0.40059823\n",
      "Validation score: 0.613409\n",
      "Iteration 88, loss = 0.39130606\n",
      "Validation score: 0.618421\n",
      "Iteration 89, loss = 0.38685086\n",
      "Validation score: 0.612782\n",
      "Iteration 90, loss = 0.38071555\n",
      "Validation score: 0.620927\n",
      "Iteration 91, loss = 0.37327414\n",
      "Validation score: 0.632206\n",
      "Iteration 92, loss = 0.36138153\n",
      "Validation score: 0.615915\n",
      "Iteration 93, loss = 0.36207788\n",
      "Validation score: 0.615288\n",
      "Iteration 94, loss = 0.35078262\n",
      "Validation score: 0.621554\n",
      "Iteration 95, loss = 0.34514230\n",
      "Validation score: 0.617794\n",
      "Iteration 96, loss = 0.34416950\n",
      "Validation score: 0.610902\n",
      "Iteration 97, loss = 0.33973096\n",
      "Validation score: 0.612782\n",
      "Iteration 98, loss = 0.33521845\n",
      "Validation score: 0.619674\n",
      "Iteration 99, loss = 0.31813471\n",
      "Validation score: 0.622807\n",
      "Iteration 100, loss = 0.31562742\n",
      "Validation score: 0.622807\n",
      "Iteration 101, loss = 0.31057001\n",
      "Validation score: 0.619048\n",
      "Iteration 102, loss = 0.31477481\n",
      "Validation score: 0.614662\n",
      "Iteration 103, loss = 0.30726140\n",
      "Validation score: 0.619674\n",
      "Iteration 104, loss = 0.29548166\n",
      "Validation score: 0.625940\n",
      "Iteration 105, loss = 0.29548340\n",
      "Validation score: 0.627820\n",
      "Iteration 106, loss = 0.28008164\n",
      "Validation score: 0.627820\n",
      "Iteration 107, loss = 0.28046218\n",
      "Validation score: 0.629699\n",
      "Validation score did not improve more than tol=0.000100 for 15 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(batch_size=128, early_stopping=True,\n",
       "              hidden_layer_sizes=(256, 128), learning_rate='adaptive',\n",
       "              max_iter=400, n_iter_no_change=15, random_state=42,\n",
       "              validation_fraction=0.15, verbose=True)"
      ],
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(batch_size=128, early_stopping=True,\n",
       "              hidden_layer_sizes=(256, 128), learning_rate=&#x27;adaptive&#x27;,\n",
       "              max_iter=400, n_iter_no_change=15, random_state=42,\n",
       "              validation_fraction=0.15, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(batch_size=128, early_stopping=True,\n",
       "              hidden_layer_sizes=(256, 128), learning_rate=&#x27;adaptive&#x27;,\n",
       "              max_iter=400, n_iter_no_change=15, random_state=42,\n",
       "              validation_fraction=0.15, verbose=True)</pre></div></div></div></div></div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "val_pred = mlp.predict(X_val)\n",
    "val_proba = mlp.predict_proba(X_val)\n",
    "\n",
    "# LabelBinarizer for multiclass ROC/PR\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y)\n",
    "\n",
    "y_val_bin = lb.transform(y_val)\n",
    "\n",
    "# Metrics\n",
    "val_acc = accuracy_score(y_val, val_pred)\n",
    "val_f1_macro = f1_score(y_val, val_pred, average='macro')\n",
    "val_f1_micro = f1_score(y_val, val_pred, average='micro')\n",
    "val_f1_weighted = f1_score(y_val, val_pred, average='weighted')\n",
    "\n",
    "# ROC-AUC\n",
    "val_roc_auc = roc_auc_score(y_val_bin, val_proba, multi_class=\"ovr\", average=\"macro\")\n",
    "\n",
    "# PR-AUC\n",
    "pr_auc_list = []\n",
    "for i in range(len(lb.classes_)):\n",
    "    pr_auc_list.append(average_precision_score(y_val_bin[:, i], val_proba[:, i]))\n",
    "val_pr_auc_macro = np.mean(pr_auc_list)\n",
    "\n",
    "print(\"\\n===== [VAL RESULTS] =====\")\n",
    "print(f\"Accuracy       : {val_acc:.4f}\")\n",
    "print(f\"F1 (macro)     : {val_f1_macro:.4f}\")\n",
    "print(f\"F1 (micro)     : {val_f1_micro:.4f}\")\n",
    "print(f\"F1 (weighted)  : {val_f1_weighted:.4f}\")\n",
    "print(f\"ROC-AUC (OvR)  : {val_roc_auc:.4f}\")\n",
    "print(f\"PR-AUC (macro) : {val_pr_auc_macro:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, val_pred, digits=4))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0lPL_oeB8ulk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762877029372,
     "user_tz": -540,
     "elapsed": 99,
     "user": {
      "displayName": "­신은서(엘텍공과대학 소프트웨어학부)",
      "userId": "09254021355381794528"
     }
    },
    "outputId": "c427f0b5-3c73-4ac2-836b-790107c8f40c",
    "ExecuteTime": {
     "end_time": "2025-11-28T02:37:30.245163Z",
     "start_time": "2025-11-28T02:37:30.099096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== [VAL RESULTS] =====\n",
      "Accuracy       : 0.6305\n",
      "F1 (macro)     : 0.6297\n",
      "F1 (micro)     : 0.6305\n",
      "F1 (weighted)  : 0.6297\n",
      "ROC-AUC (OvR)  : 0.9792\n",
      "PR-AUC (macro) : 0.6817\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4828    0.5000    0.4912        28\n",
      "           1     0.6667    0.5714    0.6154        28\n",
      "           2     0.5556    0.7143    0.6250        28\n",
      "           3     0.6800    0.6071    0.6415        28\n",
      "           4     0.6364    0.5000    0.5600        28\n",
      "           5     0.5600    0.5000    0.5283        28\n",
      "           6     0.6000    0.7500    0.6667        28\n",
      "           7     0.5758    0.6786    0.6230        28\n",
      "           8     0.5152    0.6071    0.5574        28\n",
      "           9     0.5667    0.6071    0.5862        28\n",
      "          10     0.4839    0.5357    0.5085        28\n",
      "          11     0.7037    0.6786    0.6909        28\n",
      "          12     0.8947    0.6071    0.7234        28\n",
      "          13     0.4211    0.2857    0.3404        28\n",
      "          14     0.4872    0.6786    0.5672        28\n",
      "          15     0.4412    0.5357    0.4839        28\n",
      "          16     0.8462    0.7857    0.8148        28\n",
      "          17     0.3889    0.2500    0.3043        28\n",
      "          18     0.7742    0.8571    0.8136        28\n",
      "          19     0.4242    0.5000    0.4590        28\n",
      "          20     0.8929    0.8929    0.8929        28\n",
      "          21     0.7083    0.6071    0.6538        28\n",
      "          22     0.5385    0.5000    0.5185        28\n",
      "          23     0.8947    0.6071    0.7234        28\n",
      "          24     0.3600    0.3214    0.3396        28\n",
      "          25     0.4286    0.4286    0.4286        28\n",
      "          26     0.6061    0.7143    0.6557        28\n",
      "          27     0.5758    0.6786    0.6230        28\n",
      "          28     0.7000    0.7500    0.7241        28\n",
      "          29     0.7742    0.8571    0.8136        28\n",
      "          30     0.6667    0.6429    0.6545        28\n",
      "          31     0.5000    0.3929    0.4400        28\n",
      "          32     0.4750    0.6786    0.5588        28\n",
      "          33     0.8636    0.6786    0.7600        28\n",
      "          34     0.4333    0.4643    0.4483        28\n",
      "          35     0.7407    0.7143    0.7273        28\n",
      "          36     0.6944    0.8929    0.7812        28\n",
      "          37     0.5000    0.3214    0.3913        28\n",
      "          38     0.6957    0.5714    0.6275        28\n",
      "          39     0.8000    0.7143    0.7547        28\n",
      "          40     0.4667    0.5000    0.4828        28\n",
      "          41     0.7333    0.7857    0.7586        28\n",
      "          42     0.5000    0.6071    0.5484        28\n",
      "          43     0.6000    0.6429    0.6207        28\n",
      "          44     0.9655    1.0000    0.9825        28\n",
      "          45     0.3103    0.3214    0.3158        28\n",
      "          46     0.6333    0.6786    0.6552        28\n",
      "          47     0.4231    0.3929    0.4074        28\n",
      "          48     0.5938    0.6786    0.6333        28\n",
      "          49     0.5625    0.6429    0.6000        28\n",
      "          50     0.7333    0.7857    0.7586        28\n",
      "          51     0.6667    0.7143    0.6897        28\n",
      "          52     0.5000    0.7143    0.5882        28\n",
      "          53     0.8000    0.7143    0.7547        28\n",
      "          54     0.4706    0.5714    0.5161        28\n",
      "          55     0.6471    0.7857    0.7097        28\n",
      "          56     0.7333    0.7857    0.7586        28\n",
      "          57     0.8500    0.6071    0.7083        28\n",
      "          58     0.8750    0.7500    0.8077        28\n",
      "          59     1.0000    0.9286    0.9630        28\n",
      "          60     0.7391    0.6071    0.6667        28\n",
      "          61     0.5385    0.5000    0.5185        28\n",
      "          62     0.9167    0.7857    0.8462        28\n",
      "          63     0.5200    0.4643    0.4906        28\n",
      "          64     0.5455    0.6429    0.5902        28\n",
      "          65     0.7222    0.4643    0.5652        28\n",
      "          66     0.7391    0.6071    0.6667        28\n",
      "          67     0.8500    0.6071    0.7083        28\n",
      "          68     0.4103    0.5714    0.4776        28\n",
      "          69     0.7273    0.5714    0.6400        28\n",
      "          70     0.7586    0.7857    0.7719        28\n",
      "          71     0.5312    0.6071    0.5667        28\n",
      "          72     0.6500    0.4643    0.5417        28\n",
      "          73     0.7941    0.9643    0.8710        28\n",
      "          74     0.5000    0.6071    0.5484        28\n",
      "          75     0.9000    0.9643    0.9310        28\n",
      "          76     0.8571    0.8571    0.8571        28\n",
      "          77     0.3750    0.6429    0.4737        28\n",
      "          78     0.7407    0.7143    0.7273        28\n",
      "          79     0.5000    0.2143    0.3000        28\n",
      "          80     0.8214    0.8214    0.8214        28\n",
      "          81     0.7600    0.6786    0.7170        28\n",
      "          82     0.6667    0.3571    0.4651        28\n",
      "          83     0.3243    0.4286    0.3692        28\n",
      "          84     0.6897    0.7143    0.7018        28\n",
      "          85     1.0000    0.7500    0.8571        28\n",
      "          86     0.8000    0.8571    0.8276        28\n",
      "          87     0.7586    0.7857    0.7719        28\n",
      "          88     0.5000    0.6429    0.5625        28\n",
      "          89     0.5714    0.4286    0.4898        28\n",
      "          90     0.7308    0.6786    0.7037        28\n",
      "          91     0.5806    0.6429    0.6102        28\n",
      "          92     0.5600    0.5000    0.5283        28\n",
      "          93     0.8333    0.7143    0.7692        28\n",
      "          94     0.5200    0.4643    0.4906        28\n",
      "\n",
      "    accuracy                         0.6305      2660\n",
      "   macro avg     0.6426    0.6305    0.6297      2660\n",
      "weighted avg     0.6426    0.6305    0.6297      2660\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "test_pred = mlp.predict(X_test)\n",
    "test_proba = mlp.predict_proba(X_test)\n",
    "\n",
    "y_test_bin = lb.transform(y_test)\n",
    "\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "test_f1_macro = f1_score(y_test, test_pred, average='macro')\n",
    "test_f1_micro = f1_score(y_test, test_pred, average='micro')\n",
    "test_f1_weighted = f1_score(y_test, test_pred, average='weighted')\n",
    "\n",
    "test_roc_auc = roc_auc_score(y_test_bin, test_proba, multi_class=\"ovr\", average=\"macro\")\n",
    "\n",
    "pr_auc_list_test = []\n",
    "for i in range(len(lb.classes_)):\n",
    "    pr_auc_list_test.append(average_precision_score(y_test_bin[:, i], test_proba[:, i]))\n",
    "test_pr_auc_macro = np.mean(pr_auc_list_test)\n",
    "\n",
    "print(\"\\n===== [TEST RESULTS] =====\")\n",
    "print(f\"Accuracy       : {test_acc:.4f}\")\n",
    "print(f\"F1 (macro)     : {test_f1_macro:.4f}\")\n",
    "print(f\"F1 (micro)     : {test_f1_micro:.4f}\")\n",
    "print(f\"F1 (weighted)  : {test_f1_weighted:.4f}\")\n",
    "print(f\"ROC-AUC (OvR)  : {test_roc_auc:.4f}\")\n",
    "print(f\"PR-AUC (macro) : {test_pr_auc_macro:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Test):\")\n",
    "print(classification_report(y_test, test_pred, digits=4))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kil1Jf8U8una",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1762877040053,
     "user_tz": -540,
     "elapsed": 228,
     "user": {
      "displayName": "­신은서(엘텍공과대학 소프트웨어학부)",
      "userId": "09254021355381794528"
     }
    },
    "outputId": "1de8e9b0-3cc1-4ae6-a12b-fb58f5a773e9",
    "ExecuteTime": {
     "end_time": "2025-11-28T02:37:30.462029Z",
     "start_time": "2025-11-28T02:37:30.258763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== [TEST RESULTS] =====\n",
      "Accuracy       : 0.6396\n",
      "F1 (macro)     : 0.6376\n",
      "F1 (micro)     : 0.6396\n",
      "F1 (weighted)  : 0.6376\n",
      "ROC-AUC (OvR)  : 0.9774\n",
      "PR-AUC (macro) : 0.6825\n",
      "\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5862    0.5667    0.5763        60\n",
      "           1     0.7705    0.7833    0.7769        60\n",
      "           2     0.6875    0.7333    0.7097        60\n",
      "           3     0.6545    0.6000    0.6261        60\n",
      "           4     0.7255    0.6167    0.6667        60\n",
      "           5     0.4762    0.5000    0.4878        60\n",
      "           6     0.6164    0.7500    0.6767        60\n",
      "           7     0.6939    0.5667    0.6239        60\n",
      "           8     0.4923    0.5333    0.5120        60\n",
      "           9     0.4737    0.4500    0.4615        60\n",
      "          10     0.6415    0.5667    0.6018        60\n",
      "          11     0.6290    0.6500    0.6393        60\n",
      "          12     0.8235    0.7000    0.7568        60\n",
      "          13     0.4035    0.3833    0.3932        60\n",
      "          14     0.4658    0.5667    0.5113        60\n",
      "          15     0.4675    0.6000    0.5255        60\n",
      "          16     0.6481    0.5833    0.6140        60\n",
      "          17     0.5185    0.4667    0.4912        60\n",
      "          18     0.6508    0.6833    0.6667        60\n",
      "          19     0.4935    0.6333    0.5547        60\n",
      "          20     0.8485    0.9333    0.8889        60\n",
      "          21     0.6607    0.6167    0.6379        60\n",
      "          22     0.5000    0.4500    0.4737        60\n",
      "          23     0.7429    0.4333    0.5474        60\n",
      "          24     0.3191    0.2500    0.2804        60\n",
      "          25     0.4521    0.5500    0.4962        60\n",
      "          26     0.6308    0.6833    0.6560        60\n",
      "          27     0.4714    0.5500    0.5077        60\n",
      "          28     0.6774    0.7000    0.6885        60\n",
      "          29     0.7612    0.8500    0.8031        60\n",
      "          30     0.7119    0.7000    0.7059        60\n",
      "          31     0.6667    0.6333    0.6496        60\n",
      "          32     0.5517    0.8000    0.6531        60\n",
      "          33     0.7797    0.7667    0.7731        60\n",
      "          34     0.5286    0.6167    0.5692        60\n",
      "          35     0.7843    0.6667    0.7207        60\n",
      "          36     0.6575    0.8000    0.7218        60\n",
      "          37     0.5400    0.4500    0.4909        60\n",
      "          38     0.6458    0.5167    0.5741        60\n",
      "          39     0.8085    0.6333    0.7103        60\n",
      "          40     0.6327    0.5167    0.5688        60\n",
      "          41     0.6962    0.9167    0.7914        60\n",
      "          42     0.5694    0.6833    0.6212        60\n",
      "          43     0.6885    0.7000    0.6942        60\n",
      "          44     0.9333    0.9333    0.9333        60\n",
      "          45     0.4754    0.4833    0.4793        60\n",
      "          46     0.6610    0.6500    0.6555        60\n",
      "          47     0.5410    0.5500    0.5455        60\n",
      "          48     0.5283    0.4667    0.4956        60\n",
      "          49     0.6230    0.6333    0.6281        60\n",
      "          50     0.6393    0.6500    0.6446        60\n",
      "          51     0.6508    0.6833    0.6667        60\n",
      "          52     0.5846    0.6333    0.6080        60\n",
      "          53     0.9038    0.7833    0.8393        60\n",
      "          54     0.5758    0.6333    0.6032        60\n",
      "          55     0.6607    0.6167    0.6379        60\n",
      "          56     0.7966    0.7833    0.7899        60\n",
      "          57     0.7778    0.7000    0.7368        60\n",
      "          58     0.8596    0.8167    0.8376        60\n",
      "          59     0.9434    0.8333    0.8850        60\n",
      "          60     0.6327    0.5167    0.5688        60\n",
      "          61     0.5833    0.5833    0.5833        60\n",
      "          62     0.5862    0.5667    0.5763        60\n",
      "          63     0.5636    0.5167    0.5391        60\n",
      "          64     0.6029    0.6833    0.6406        60\n",
      "          65     0.6585    0.4500    0.5347        60\n",
      "          66     0.6620    0.7833    0.7176        60\n",
      "          67     0.8214    0.7667    0.7931        60\n",
      "          68     0.5645    0.5833    0.5738        60\n",
      "          69     0.7091    0.6500    0.6783        60\n",
      "          70     0.7536    0.8667    0.8062        60\n",
      "          71     0.6250    0.6667    0.6452        60\n",
      "          72     0.6087    0.4667    0.5283        60\n",
      "          73     0.8030    0.8833    0.8413        60\n",
      "          74     0.5932    0.5833    0.5882        60\n",
      "          75     0.7606    0.9000    0.8244        60\n",
      "          76     0.8413    0.8833    0.8618        60\n",
      "          77     0.3929    0.5500    0.4583        60\n",
      "          78     0.7174    0.5500    0.6226        60\n",
      "          79     0.5625    0.3000    0.3913        60\n",
      "          80     0.7925    0.7000    0.7434        60\n",
      "          81     0.6271    0.6167    0.6218        60\n",
      "          82     0.7429    0.4333    0.5474        60\n",
      "          83     0.3929    0.5500    0.4583        60\n",
      "          84     0.7937    0.8333    0.8130        60\n",
      "          85     0.8491    0.7500    0.7965        60\n",
      "          86     0.7846    0.8500    0.8160        60\n",
      "          87     0.7538    0.8167    0.7840        60\n",
      "          88     0.5636    0.5167    0.5391        60\n",
      "          89     0.4583    0.3667    0.4074        60\n",
      "          90     0.6212    0.6833    0.6508        60\n",
      "          91     0.5921    0.7500    0.6618        60\n",
      "          92     0.5532    0.4333    0.4860        60\n",
      "          93     0.7606    0.9000    0.8244        60\n",
      "          94     0.5286    0.6167    0.5692        60\n",
      "\n",
      "    accuracy                         0.6396      5700\n",
      "   macro avg     0.6448    0.6396    0.6376      5700\n",
      "weighted avg     0.6448    0.6396    0.6376      5700\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ]
}
