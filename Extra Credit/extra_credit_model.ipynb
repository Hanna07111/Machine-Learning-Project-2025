{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "B_sWX8sdEv08"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tpwQWFs7t2R8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "37e635f4-6363-4972-92ab-49fc5d8a5786"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       all_total_packets     all_num_in    all_num_out  all_incoming_ratio  \\\n",
              "count      172085.000000  172085.000000  172085.000000       172085.000000   \n",
              "mean          981.683465     904.591882      77.091583            0.836945   \n",
              "std          1136.425769    1110.021706     120.572424            0.273002   \n",
              "min             1.000000       0.000000       0.000000            0.000000   \n",
              "25%           200.000000     148.000000       0.000000            0.807377   \n",
              "50%           566.000000     489.000000      23.000000            0.970732   \n",
              "75%          1334.000000    1233.000000     112.000000            1.000000   \n",
              "max          9122.000000    9047.000000    2898.000000            1.000000   \n",
              "\n",
              "        all_duration  all_pkts_per_sec   all_ipt_mean    all_ipt_std  \\\n",
              "count  172085.000000     172085.000000  172085.000000  172085.000000   \n",
              "mean       13.747864        148.091007       0.043660       0.261023   \n",
              "std        13.517647        309.822410       0.150884       0.557011   \n",
              "min         0.000000          0.000000       0.000000       0.000000   \n",
              "25%         4.376000         27.644431       0.006332       0.049659   \n",
              "50%         9.054303         67.227543       0.014870       0.113602   \n",
              "75%        18.017591        157.631111       0.036188       0.263711   \n",
              "max        85.868500      19200.000429      11.083250      18.817187   \n",
              "\n",
              "         all_ipt_max    all_ipt_q75  ...  firstT_packets  firstT_in_ratio  \\\n",
              "count  172085.000000  172085.000000  ...   172085.000000    172085.000000   \n",
              "mean        4.281876       0.016502  ...      416.821263         0.778056   \n",
              "std         6.606039       0.131627  ...      622.884015         0.360676   \n",
              "min         0.000000       0.000000  ...        1.000000         0.000000   \n",
              "25%         0.957000       0.001000  ...       50.000000         0.717391   \n",
              "50%         1.946500       0.002000  ...      177.000000         1.000000   \n",
              "75%         4.521112       0.004750  ...      521.000000         1.000000   \n",
              "max        80.194999      19.783500  ...     8252.000000         1.000000   \n",
              "\n",
              "       firstT_out_ratio  firstT_ipt_mean  firstT_ipt_std  first30_in_ratio  \\\n",
              "count     172085.000000    172085.000000   172085.000000     172085.000000   \n",
              "mean           0.221944         0.084556        0.167037          0.756304   \n",
              "std            0.360676         0.195728        0.220571          0.399859   \n",
              "min            0.000000         0.000000        0.000000          0.000000   \n",
              "25%            0.000000         0.005553        0.037173          0.633333   \n",
              "50%            0.000000         0.014721        0.086495          1.000000   \n",
              "75%            0.282609         0.064740        0.208494          1.000000   \n",
              "max            1.000000         4.974000        2.449500          1.000000   \n",
              "\n",
              "       first30_out_ratio   firstin_time  firstin_pkts_before          label  \n",
              "count      172085.000000  172085.000000          172085.0000  172085.000000  \n",
              "mean            0.243696       1.020962              14.8187      70.772078  \n",
              "std             0.399859       2.836852              40.2883      30.875847  \n",
              "min             0.000000       0.000000               0.0000       0.000000  \n",
              "25%             0.000000       0.000000               0.0000      47.000000  \n",
              "50%             0.000000       0.000000               0.0000      93.000000  \n",
              "75%             0.366667       0.000000               2.0000      95.000000  \n",
              "max             1.000000      56.626537            2231.0000      95.000000  \n",
              "\n",
              "[8 rows x 50 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-89292ce9-8d78-4e58-a22e-1f149c664069\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>all_total_packets</th>\n",
              "      <th>all_num_in</th>\n",
              "      <th>all_num_out</th>\n",
              "      <th>all_incoming_ratio</th>\n",
              "      <th>all_duration</th>\n",
              "      <th>all_pkts_per_sec</th>\n",
              "      <th>all_ipt_mean</th>\n",
              "      <th>all_ipt_std</th>\n",
              "      <th>all_ipt_max</th>\n",
              "      <th>all_ipt_q75</th>\n",
              "      <th>...</th>\n",
              "      <th>firstT_packets</th>\n",
              "      <th>firstT_in_ratio</th>\n",
              "      <th>firstT_out_ratio</th>\n",
              "      <th>firstT_ipt_mean</th>\n",
              "      <th>firstT_ipt_std</th>\n",
              "      <th>first30_in_ratio</th>\n",
              "      <th>first30_out_ratio</th>\n",
              "      <th>firstin_time</th>\n",
              "      <th>firstin_pkts_before</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>172085.000000</td>\n",
              "      <td>172085.000000</td>\n",
              "      <td>172085.000000</td>\n",
              "      <td>172085.000000</td>\n",
              "      <td>172085.000000</td>\n",
              "      <td>172085.000000</td>\n",
              "      <td>172085.000000</td>\n",
              "      <td>172085.000000</td>\n",
              "      <td>172085.000000</td>\n",
              "      <td>172085.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>172085.000000</td>\n",
              "      <td>172085.000000</td>\n",
              "      <td>172085.000000</td>\n",
              "      <td>172085.000000</td>\n",
              "      <td>172085.000000</td>\n",
              "      <td>172085.000000</td>\n",
              "      <td>172085.000000</td>\n",
              "      <td>172085.000000</td>\n",
              "      <td>172085.0000</td>\n",
              "      <td>172085.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>981.683465</td>\n",
              "      <td>904.591882</td>\n",
              "      <td>77.091583</td>\n",
              "      <td>0.836945</td>\n",
              "      <td>13.747864</td>\n",
              "      <td>148.091007</td>\n",
              "      <td>0.043660</td>\n",
              "      <td>0.261023</td>\n",
              "      <td>4.281876</td>\n",
              "      <td>0.016502</td>\n",
              "      <td>...</td>\n",
              "      <td>416.821263</td>\n",
              "      <td>0.778056</td>\n",
              "      <td>0.221944</td>\n",
              "      <td>0.084556</td>\n",
              "      <td>0.167037</td>\n",
              "      <td>0.756304</td>\n",
              "      <td>0.243696</td>\n",
              "      <td>1.020962</td>\n",
              "      <td>14.8187</td>\n",
              "      <td>70.772078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1136.425769</td>\n",
              "      <td>1110.021706</td>\n",
              "      <td>120.572424</td>\n",
              "      <td>0.273002</td>\n",
              "      <td>13.517647</td>\n",
              "      <td>309.822410</td>\n",
              "      <td>0.150884</td>\n",
              "      <td>0.557011</td>\n",
              "      <td>6.606039</td>\n",
              "      <td>0.131627</td>\n",
              "      <td>...</td>\n",
              "      <td>622.884015</td>\n",
              "      <td>0.360676</td>\n",
              "      <td>0.360676</td>\n",
              "      <td>0.195728</td>\n",
              "      <td>0.220571</td>\n",
              "      <td>0.399859</td>\n",
              "      <td>0.399859</td>\n",
              "      <td>2.836852</td>\n",
              "      <td>40.2883</td>\n",
              "      <td>30.875847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>200.000000</td>\n",
              "      <td>148.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.807377</td>\n",
              "      <td>4.376000</td>\n",
              "      <td>27.644431</td>\n",
              "      <td>0.006332</td>\n",
              "      <td>0.049659</td>\n",
              "      <td>0.957000</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>...</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>0.717391</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005553</td>\n",
              "      <td>0.037173</td>\n",
              "      <td>0.633333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>47.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>566.000000</td>\n",
              "      <td>489.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>0.970732</td>\n",
              "      <td>9.054303</td>\n",
              "      <td>67.227543</td>\n",
              "      <td>0.014870</td>\n",
              "      <td>0.113602</td>\n",
              "      <td>1.946500</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>...</td>\n",
              "      <td>177.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014721</td>\n",
              "      <td>0.086495</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>93.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1334.000000</td>\n",
              "      <td>1233.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>18.017591</td>\n",
              "      <td>157.631111</td>\n",
              "      <td>0.036188</td>\n",
              "      <td>0.263711</td>\n",
              "      <td>4.521112</td>\n",
              "      <td>0.004750</td>\n",
              "      <td>...</td>\n",
              "      <td>521.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.282609</td>\n",
              "      <td>0.064740</td>\n",
              "      <td>0.208494</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.366667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.0000</td>\n",
              "      <td>95.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>9122.000000</td>\n",
              "      <td>9047.000000</td>\n",
              "      <td>2898.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>85.868500</td>\n",
              "      <td>19200.000429</td>\n",
              "      <td>11.083250</td>\n",
              "      <td>18.817187</td>\n",
              "      <td>80.194999</td>\n",
              "      <td>19.783500</td>\n",
              "      <td>...</td>\n",
              "      <td>8252.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.974000</td>\n",
              "      <td>2.449500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>56.626537</td>\n",
              "      <td>2231.0000</td>\n",
              "      <td>95.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 50 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89292ce9-8d78-4e58-a22e-1f149c664069')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-89292ce9-8d78-4e58-a22e-1f149c664069 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-89292ce9-8d78-4e58-a22e-1f149c664069');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6d2cd749-91a3-46bc-b7ce-670cfe9df369\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6d2cd749-91a3-46bc-b7ce-670cfe9df369')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6d2cd749-91a3-46bc-b7ce-670cfe9df369 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "all_df = pd.read_csv('/content/all_features.csv')\n",
        "\n",
        "all_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "def train_and_test_with_XGBoost(train_df, test_df):\n",
        "\n",
        "  # ===================================================\n",
        "  # Closed world (95 제거)\n",
        "  # ===================================================\n",
        "  train_cw = train_df[train_df['label'] != 95].copy()\n",
        "  test_cw  = test_df[test_df['label'] != 95].copy()\n",
        "\n",
        "  y_train_cw = train_cw['label']\n",
        "  X_train_cw = train_cw.drop(columns=['label'])\n",
        "\n",
        "  y_test_cw = test_cw['label']\n",
        "  X_test_cw = test_cw.drop(columns=['label'])\n",
        "\n",
        "  print(\"[INFO] Train shape:\", X_train_cw.shape)\n",
        "  print(\"[INFO] Test shape:\", X_test_cw.shape)\n",
        "\n",
        "  print(\"\\n================ Closed-World (95-class) ================\\n\")\n",
        "\n",
        "  num_classes = y_train_cw.nunique()\n",
        "  print(\"[INFO] Classes:\", num_classes)\n",
        "\n",
        "  model_closed = XGBClassifier(\n",
        "      objective='multi:softprob',   # closed-world, open-multi\n",
        "      eval_metric='mlogloss',\n",
        "      tree_method='hist',\n",
        "      learning_rate=0.05,\n",
        "      n_estimators=200,\n",
        "      max_depth=6,\n",
        "      subsample=0.8,\n",
        "      colsample_bytree=0.8,\n",
        "      random_state=42,\n",
        "      device='cuda'\n",
        "  )\n",
        "\n",
        "  model_closed.fit(X_train_cw, y_train_cw)\n",
        "\n",
        "  pred_closed = model_closed.predict(X_test_cw)\n",
        "\n",
        "  print(\"[Closed-World] Accuracy:\", accuracy_score(y_test_cw, pred_closed))\n",
        "  print(classification_report(y_test_cw, pred_closed))\n",
        "\n",
        "  # ===================================================\n",
        "  # Open world\n",
        "  # ===================================================\n",
        "  X_train_ow = train_df.drop(columns=['label'])\n",
        "  y_train_ow = train_df['label']\n",
        "\n",
        "  X_test_ow = test_df.drop(columns=['label'])\n",
        "  y_test_ow = test_df['label']\n",
        "\n",
        "  print(\"[INFO] Train shape:\", X_train_ow.shape)\n",
        "  print(\"[INFO] Test shape:\", X_test_ow.shape)\n",
        "\n",
        "  # Binary labels: mon=1, unmon=0\n",
        "  y_train_ow_bin = (y_train_ow != 95).astype(int)\n",
        "  y_test_ow_bin  = (y_test_ow  != 95).astype(int)\n",
        "\n",
        "  print(\"[INFO] Binary class ratio:\", np.bincount(y_train_ow_bin))\n",
        "\n",
        "  # Open world binary\n",
        "\n",
        "  model_binary = XGBClassifier(\n",
        "      tree_method='hist',\n",
        "      learning_rate=0.05,\n",
        "      n_estimators=200,\n",
        "      max_depth=6,\n",
        "      subsample=0.8,\n",
        "      colsample_bytree=0.8,\n",
        "      objective='binary:logistic',\n",
        "      eval_metric='logloss',\n",
        "      random_state=42,\n",
        "      device='cuda'\n",
        "  )\n",
        "\n",
        "  model_binary.fit(X_train_ow, y_train_ow_bin)\n",
        "\n",
        "  pred_binary = (model_binary.predict_proba(X_test_ow)[:,1] > 0.5).astype(int)\n",
        "\n",
        "  print(\"[Binary] Accuracy:\", accuracy_score(y_test_ow_bin, pred_binary))\n",
        "  print(classification_report(y_test_ow_bin, pred_binary))\n",
        "\n",
        "  # Open world multi\n",
        "\n",
        "  print(\"\\n================ Open-World Multiclass (0~95) ================\\n\")\n",
        "\n",
        "  num_classes = y_train_ow.nunique()\n",
        "  print(\"[INFO] Classes:\", num_classes)\n",
        "\n",
        "  model_multi = XGBClassifier(\n",
        "      objective='multi:softprob',   # closed-world, open-multi\n",
        "      eval_metric='mlogloss',\n",
        "      tree_method='hist',\n",
        "      learning_rate=0.05,\n",
        "      n_estimators=200,\n",
        "      max_depth=6,\n",
        "      subsample=0.8,\n",
        "      colsample_bytree=0.8,\n",
        "      random_state=42,\n",
        "      num_class=num_classes,\n",
        "      device='cuda'\n",
        "  )\n",
        "\n",
        "  model_multi.fit(X_train_ow, y_train_ow)\n",
        "\n",
        "  pred_multi = model_multi.predict(X_test_ow)\n",
        "\n",
        "  print(\"[Open-Multi] Accuracy:\", accuracy_score(y_test_ow, pred_multi))\n",
        "  print(classification_report(y_test_ow, pred_multi))\n"
      ],
      "metadata": {
        "id": "hE336aZwE4vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 기본"
      ],
      "metadata": {
        "id": "B_sWX8sdEv08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train/test split\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(all_df, test_size=0.25, stratify=all_df['label'])\n",
        "\n",
        "train_df.describe()\n",
        "test_df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "yOSsPHGF9ImL",
        "outputId": "c806720b-66b2-4334-facb-d8479fc34063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       all_total_packets    all_num_in   all_num_out  all_incoming_ratio  \\\n",
              "count       43022.000000  43022.000000  43022.000000        43022.000000   \n",
              "mean          982.707429    906.280740     76.426689            0.836689   \n",
              "std          1143.423459   1117.471875    120.091083            0.274499   \n",
              "min             1.000000      0.000000      0.000000            0.000000   \n",
              "25%           199.000000    148.000000      0.000000            0.808896   \n",
              "50%           564.000000    487.000000     22.000000            0.971150   \n",
              "75%          1332.000000   1231.000000    112.000000            1.000000   \n",
              "max          8705.000000   8693.000000   2231.000000            1.000000   \n",
              "\n",
              "       all_duration  all_pkts_per_sec  all_ipt_mean   all_ipt_std  \\\n",
              "count  43022.000000      43022.000000  43022.000000  43022.000000   \n",
              "mean      13.664851        149.164484      0.043867      0.259641   \n",
              "std       13.458357        293.470450      0.145212      0.547574   \n",
              "min        0.000000          0.000000      0.000000      0.000000   \n",
              "25%        4.341700         27.672925      0.006291      0.049273   \n",
              "50%        8.990854         67.656118      0.014782      0.113404   \n",
              "75%       17.902986        158.666593      0.036141      0.260508   \n",
              "max       84.300003      15333.333993      6.037500     15.452378   \n",
              "\n",
              "        all_ipt_max   all_ipt_q75  ...  firstT_packets  firstT_in_ratio  \\\n",
              "count  43022.000000  43022.000000  ...    43022.000000     43022.000000   \n",
              "mean       4.248674      0.017037  ...      419.447562         0.778474   \n",
              "std        6.538871      0.128302  ...      631.895565         0.360985   \n",
              "min        0.000000      0.000000  ...        1.000000         0.000000   \n",
              "25%        0.947000      0.001000  ...       50.000000         0.720383   \n",
              "50%        1.926649      0.002000  ...      178.000000         1.000000   \n",
              "75%        4.484250      0.004750  ...      524.000000         1.000000   \n",
              "max       61.061724     11.450812  ...     7791.000000         1.000000   \n",
              "\n",
              "       firstT_out_ratio  firstT_ipt_mean  firstT_ipt_std  first30_in_ratio  \\\n",
              "count      43022.000000     43022.000000    43022.000000      43022.000000   \n",
              "mean           0.221526         0.085440        0.167148          0.757104   \n",
              "std            0.360985         0.200724        0.222801          0.399936   \n",
              "min            0.000000         0.000000        0.000000          0.000000   \n",
              "25%            0.000000         0.005511        0.037018          0.633333   \n",
              "50%            0.000000         0.014661        0.085824          1.000000   \n",
              "75%            0.279617         0.064103        0.206990          1.000000   \n",
              "max            1.000000         4.590500        2.438000          1.000000   \n",
              "\n",
              "       first30_out_ratio  firstin_time  firstin_pkts_before         label  \n",
              "count       43022.000000  43022.000000         43022.000000  43022.000000  \n",
              "mean            0.242896      1.021297            14.750151     70.775231  \n",
              "std             0.399936      2.862344            41.493014     30.873915  \n",
              "min             0.000000      0.000000             0.000000      0.000000  \n",
              "25%             0.000000      0.000000             0.000000     47.000000  \n",
              "50%             0.000000      0.000000             0.000000     93.000000  \n",
              "75%             0.366667      0.000000             1.000000     95.000000  \n",
              "max             1.000000     54.044556          2231.000000     95.000000  \n",
              "\n",
              "[8 rows x 50 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7c979964-b4ba-4bae-852e-208ea9fd91ff\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>all_total_packets</th>\n",
              "      <th>all_num_in</th>\n",
              "      <th>all_num_out</th>\n",
              "      <th>all_incoming_ratio</th>\n",
              "      <th>all_duration</th>\n",
              "      <th>all_pkts_per_sec</th>\n",
              "      <th>all_ipt_mean</th>\n",
              "      <th>all_ipt_std</th>\n",
              "      <th>all_ipt_max</th>\n",
              "      <th>all_ipt_q75</th>\n",
              "      <th>...</th>\n",
              "      <th>firstT_packets</th>\n",
              "      <th>firstT_in_ratio</th>\n",
              "      <th>firstT_out_ratio</th>\n",
              "      <th>firstT_ipt_mean</th>\n",
              "      <th>firstT_ipt_std</th>\n",
              "      <th>first30_in_ratio</th>\n",
              "      <th>first30_out_ratio</th>\n",
              "      <th>firstin_time</th>\n",
              "      <th>firstin_pkts_before</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>43022.000000</td>\n",
              "      <td>43022.000000</td>\n",
              "      <td>43022.000000</td>\n",
              "      <td>43022.000000</td>\n",
              "      <td>43022.000000</td>\n",
              "      <td>43022.000000</td>\n",
              "      <td>43022.000000</td>\n",
              "      <td>43022.000000</td>\n",
              "      <td>43022.000000</td>\n",
              "      <td>43022.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>43022.000000</td>\n",
              "      <td>43022.000000</td>\n",
              "      <td>43022.000000</td>\n",
              "      <td>43022.000000</td>\n",
              "      <td>43022.000000</td>\n",
              "      <td>43022.000000</td>\n",
              "      <td>43022.000000</td>\n",
              "      <td>43022.000000</td>\n",
              "      <td>43022.000000</td>\n",
              "      <td>43022.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>982.707429</td>\n",
              "      <td>906.280740</td>\n",
              "      <td>76.426689</td>\n",
              "      <td>0.836689</td>\n",
              "      <td>13.664851</td>\n",
              "      <td>149.164484</td>\n",
              "      <td>0.043867</td>\n",
              "      <td>0.259641</td>\n",
              "      <td>4.248674</td>\n",
              "      <td>0.017037</td>\n",
              "      <td>...</td>\n",
              "      <td>419.447562</td>\n",
              "      <td>0.778474</td>\n",
              "      <td>0.221526</td>\n",
              "      <td>0.085440</td>\n",
              "      <td>0.167148</td>\n",
              "      <td>0.757104</td>\n",
              "      <td>0.242896</td>\n",
              "      <td>1.021297</td>\n",
              "      <td>14.750151</td>\n",
              "      <td>70.775231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1143.423459</td>\n",
              "      <td>1117.471875</td>\n",
              "      <td>120.091083</td>\n",
              "      <td>0.274499</td>\n",
              "      <td>13.458357</td>\n",
              "      <td>293.470450</td>\n",
              "      <td>0.145212</td>\n",
              "      <td>0.547574</td>\n",
              "      <td>6.538871</td>\n",
              "      <td>0.128302</td>\n",
              "      <td>...</td>\n",
              "      <td>631.895565</td>\n",
              "      <td>0.360985</td>\n",
              "      <td>0.360985</td>\n",
              "      <td>0.200724</td>\n",
              "      <td>0.222801</td>\n",
              "      <td>0.399936</td>\n",
              "      <td>0.399936</td>\n",
              "      <td>2.862344</td>\n",
              "      <td>41.493014</td>\n",
              "      <td>30.873915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>199.000000</td>\n",
              "      <td>148.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.808896</td>\n",
              "      <td>4.341700</td>\n",
              "      <td>27.672925</td>\n",
              "      <td>0.006291</td>\n",
              "      <td>0.049273</td>\n",
              "      <td>0.947000</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>...</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>0.720383</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005511</td>\n",
              "      <td>0.037018</td>\n",
              "      <td>0.633333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>47.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>564.000000</td>\n",
              "      <td>487.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.971150</td>\n",
              "      <td>8.990854</td>\n",
              "      <td>67.656118</td>\n",
              "      <td>0.014782</td>\n",
              "      <td>0.113404</td>\n",
              "      <td>1.926649</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>...</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014661</td>\n",
              "      <td>0.085824</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>93.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1332.000000</td>\n",
              "      <td>1231.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>17.902986</td>\n",
              "      <td>158.666593</td>\n",
              "      <td>0.036141</td>\n",
              "      <td>0.260508</td>\n",
              "      <td>4.484250</td>\n",
              "      <td>0.004750</td>\n",
              "      <td>...</td>\n",
              "      <td>524.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.279617</td>\n",
              "      <td>0.064103</td>\n",
              "      <td>0.206990</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.366667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>95.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>8705.000000</td>\n",
              "      <td>8693.000000</td>\n",
              "      <td>2231.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>84.300003</td>\n",
              "      <td>15333.333993</td>\n",
              "      <td>6.037500</td>\n",
              "      <td>15.452378</td>\n",
              "      <td>61.061724</td>\n",
              "      <td>11.450812</td>\n",
              "      <td>...</td>\n",
              "      <td>7791.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.590500</td>\n",
              "      <td>2.438000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>54.044556</td>\n",
              "      <td>2231.000000</td>\n",
              "      <td>95.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 50 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c979964-b4ba-4bae-852e-208ea9fd91ff')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7c979964-b4ba-4bae-852e-208ea9fd91ff button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7c979964-b4ba-4bae-852e-208ea9fd91ff');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-18a792f7-8776-4f95-bb55-98f73193b83c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-18a792f7-8776-4f95-bb55-98f73193b83c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-18a792f7-8776-4f95-bb55-98f73193b83c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Closed world (95 제거)\n",
        "train_cw = train_df[train_df['label'] != 95].copy()\n",
        "test_cw  = test_df[test_df['label'] != 95].copy()\n",
        "\n",
        "y_train_cw = train_cw['label']\n",
        "X_train_cw = train_cw.drop(columns=['label'])\n",
        "\n",
        "y_test_cw = test_cw['label']\n",
        "X_test_cw = test_cw.drop(columns=['label'])\n",
        "\n",
        "print(\"[INFO] Train shape:\", X_train_cw.shape)\n",
        "print(\"[INFO] Test shape:\", X_test_cw.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Be7p8RHH-ifH",
        "outputId": "9715b70d-7cbd-4e4a-c45f-eaa430dff69b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Train shape: (65278, 49)\n",
            "[INFO] Test shape: (21760, 49)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "xgb_params = dict(\n",
        "    objective='multi:softprob',   # closed-world, open-multi\n",
        "    eval_metric='mlogloss',\n",
        "    tree_method='hist',\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=200,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        ")"
      ],
      "metadata": {
        "id": "sfRpv_b3_TPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n================ Closed-World (95-class) ================\\n\")\n",
        "\n",
        "num_classes = y_train_cw.nunique()\n",
        "print(\"[INFO] Classes:\", num_classes)\n",
        "\n",
        "model_closed = XGBClassifier(\n",
        "    objective='multi:softprob',   # closed-world, open-multi\n",
        "    eval_metric='mlogloss',\n",
        "    tree_method='hist',\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=200,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    device='cuda'\n",
        ")\n",
        "\n",
        "model_closed.fit(X_train_cw, y_train_cw)\n",
        "\n",
        "pred_closed = model_closed.predict(X_test_cw)\n",
        "\n",
        "print(\"[Closed-World] Accuracy:\", accuracy_score(y_test_cw, pred_closed))\n",
        "print(classification_report(y_test_cw, pred_closed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6-dLh2qN_Kjz",
        "outputId": "63976739-9471-44ad-d9b2-c9546cc254db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================ Closed-World (95-class) ================\n",
            "\n",
            "[INFO] Classes: 95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/core.py:774: UserWarning: [06:10:58] WARNING: /workspace/src/common/error_msg.cc:62: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
            "Potential solutions:\n",
            "- Use a data structure that matches the device ordinal in the booster.\n",
            "- Set the device for booster before call to inplace_predict.\n",
            "\n",
            "This warning will only be shown once.\n",
            "\n",
            "  return func(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Closed-World] Accuracy: 0.11801470588235294\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.09      0.03      0.04       239\n",
            "           1       0.09      0.07      0.08       229\n",
            "           2       0.06      0.07      0.06       239\n",
            "           3       0.07      0.04      0.05       228\n",
            "           4       0.05      0.04      0.05       225\n",
            "           5       0.12      0.05      0.07       242\n",
            "           6       0.17      0.17      0.17       242\n",
            "           7       0.07      0.13      0.09       211\n",
            "           8       0.12      0.08      0.10       229\n",
            "           9       0.05      0.03      0.03       240\n",
            "          10       0.12      0.08      0.10       237\n",
            "          11       0.22      0.26      0.24       246\n",
            "          12       0.08      0.20      0.11       210\n",
            "          13       0.00      0.00      0.00       237\n",
            "          14       0.12      0.05      0.07       214\n",
            "          15       0.07      0.08      0.07       224\n",
            "          16       0.14      0.19      0.16       221\n",
            "          17       0.07      0.07      0.07       238\n",
            "          18       0.10      0.16      0.12       215\n",
            "          19       0.07      0.06      0.07       244\n",
            "          20       0.20      0.30      0.24       202\n",
            "          21       0.13      0.16      0.14       242\n",
            "          22       0.09      0.07      0.08       245\n",
            "          23       0.11      0.11      0.11       198\n",
            "          24       0.11      0.15      0.12       227\n",
            "          25       0.09      0.09      0.09       179\n",
            "          26       0.11      0.27      0.16       164\n",
            "          27       0.10      0.06      0.08       241\n",
            "          28       0.12      0.07      0.09       210\n",
            "          29       0.12      0.12      0.12       225\n",
            "          30       0.36      0.44      0.39       147\n",
            "          31       0.05      0.03      0.03       231\n",
            "          32       0.04      0.05      0.04       245\n",
            "          33       0.12      0.20      0.15       246\n",
            "          34       0.07      0.07      0.07       247\n",
            "          35       0.08      0.08      0.08       238\n",
            "          36       0.23      0.20      0.21       241\n",
            "          37       0.03      0.01      0.02       235\n",
            "          38       0.16      0.12      0.13       232\n",
            "          39       0.20      0.19      0.19       241\n",
            "          40       0.07      0.08      0.07       238\n",
            "          41       0.30      0.22      0.25       241\n",
            "          42       0.08      0.05      0.06       246\n",
            "          43       0.17      0.12      0.14       216\n",
            "          44       0.11      0.32      0.16       228\n",
            "          45       0.10      0.06      0.07       243\n",
            "          46       0.13      0.05      0.07       236\n",
            "          47       0.06      0.03      0.04       241\n",
            "          48       0.10      0.11      0.11       247\n",
            "          49       0.09      0.09      0.09       211\n",
            "          50       0.18      0.14      0.16       240\n",
            "          51       0.18      0.12      0.14       243\n",
            "          52       0.07      0.04      0.06       246\n",
            "          53       0.26      0.26      0.26       223\n",
            "          54       0.15      0.09      0.11       234\n",
            "          55       0.10      0.12      0.11       242\n",
            "          56       0.16      0.27      0.20       200\n",
            "          57       0.10      0.17      0.13       211\n",
            "          58       0.07      0.12      0.09       227\n",
            "          59       0.11      0.14      0.12       243\n",
            "          60       0.12      0.07      0.09       241\n",
            "          61       0.18      0.08      0.11       237\n",
            "          62       0.08      0.11      0.10       246\n",
            "          63       0.07      0.04      0.05       244\n",
            "          64       0.10      0.11      0.11       246\n",
            "          65       0.07      0.09      0.08       238\n",
            "          66       0.13      0.27      0.17       248\n",
            "          67       0.09      0.04      0.06       233\n",
            "          68       0.11      0.03      0.05       236\n",
            "          69       0.17      0.10      0.13       247\n",
            "          70       0.12      0.46      0.19       182\n",
            "          71       0.04      0.03      0.04       243\n",
            "          72       0.10      0.07      0.08       240\n",
            "          73       0.08      0.13      0.10       247\n",
            "          74       0.14      0.12      0.13       242\n",
            "          75       0.16      0.47      0.23       145\n",
            "          76       0.22      0.31      0.26       204\n",
            "          77       0.03      0.01      0.01       244\n",
            "          78       0.10      0.08      0.09       238\n",
            "          79       0.09      0.12      0.10       245\n",
            "          80       0.20      0.21      0.21       232\n",
            "          81       0.10      0.05      0.06       233\n",
            "          82       0.51      0.27      0.36       176\n",
            "          83       0.17      0.06      0.09       219\n",
            "          84       0.06      0.13      0.08       236\n",
            "          85       0.09      0.05      0.07       241\n",
            "          86       0.22      0.24      0.23       175\n",
            "          87       0.12      0.07      0.09       244\n",
            "          88       0.11      0.10      0.10       241\n",
            "          89       0.11      0.07      0.09       244\n",
            "          90       0.11      0.11      0.11       238\n",
            "          91       0.13      0.07      0.09       238\n",
            "          92       0.05      0.02      0.03       229\n",
            "          93       0.18      0.23      0.20       221\n",
            "          94       0.11      0.13      0.12       245\n",
            "\n",
            "    accuracy                           0.12     21760\n",
            "   macro avg       0.12      0.12      0.11     21760\n",
            "weighted avg       0.12      0.12      0.11     21760\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_ow = train_df.drop(columns=['label'])\n",
        "y_train_ow = train_df['label']\n",
        "\n",
        "X_test_ow = test_df.drop(columns=['label'])\n",
        "y_test_ow = test_df['label']\n",
        "\n",
        "print(\"[INFO] Train shape:\", X_train_ow.shape)\n",
        "print(\"[INFO] Test shape:\", X_test_ow.shape)\n",
        "\n",
        "# Binary labels: mon=1, unmon=0\n",
        "y_train_ow_bin = (y_train_ow != 95).astype(int)\n",
        "y_test_ow_bin  = (y_test_ow  != 95).astype(int)\n",
        "\n",
        "print(\"[INFO] Binary class ratio:\", np.bincount(y_train_ow_bin))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua0i0W039mGO",
        "outputId": "49b576d7-a057-4d44-dd55-afcf4ffaa675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Train shape: (129063, 49)\n",
            "[INFO] Test shape: (43022, 49)\n",
            "[INFO] Binary class ratio: [63785 65278]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open world binary\n",
        "\n",
        "model_binary = XGBClassifier(\n",
        "    tree_method='hist',\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=200,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    random_state=42,\n",
        "    device='cuda'\n",
        ")\n",
        "\n",
        "model_binary.fit(X_train_ow, y_train_ow_bin)\n",
        "\n",
        "pred_binary = (model_binary.predict_proba(X_test_ow)[:,1] > 0.5).astype(int)\n",
        "\n",
        "print(\"[Binary] Accuracy:\", accuracy_score(y_test_ow_bin, pred_binary))\n",
        "print(classification_report(y_test_ow_bin, pred_binary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "607g-JRMAX1U",
        "outputId": "9949030c-4ba2-44ba-a15f-036883e2c2f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Binary] Accuracy: 0.6203802705592487\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.58      0.60     21262\n",
            "           1       0.62      0.66      0.64     21760\n",
            "\n",
            "    accuracy                           0.62     43022\n",
            "   macro avg       0.62      0.62      0.62     43022\n",
            "weighted avg       0.62      0.62      0.62     43022\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open world multi\n",
        "\n",
        "print(\"\\n================ Open-World Multiclass (0~95) ================\\n\")\n",
        "\n",
        "num_classes = y_train_ow.nunique()\n",
        "print(\"[INFO] Classes:\", num_classes)\n",
        "\n",
        "model_multi = XGBClassifier(\n",
        "    objective='multi:softprob',   # closed-world, open-multi\n",
        "    eval_metric='mlogloss',\n",
        "    tree_method='hist',\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=200,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    num_class=num_classes,\n",
        "    device='cuda'\n",
        ")\n",
        "\n",
        "model_multi.fit(X_train_ow, y_train_ow)\n",
        "\n",
        "pred_multi = model_multi.predict(X_test_ow)\n",
        "\n",
        "print(\"[Open-Multi] Accuracy:\", accuracy_score(y_test_ow, pred_multi))\n",
        "print(classification_report(y_test_ow, pred_multi))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uS2MIwnHAeeg",
        "outputId": "ded28649-699a-4320-fbbf-fb7f1a14164a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================ Open-World Multiclass (0~95) ================\n",
            "\n",
            "[INFO] Classes: 96\n",
            "[Open-Multi] Accuracy: 0.5020687090325879\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.01      0.02       239\n",
            "           1       0.14      0.00      0.01       229\n",
            "           2       0.00      0.00      0.00       239\n",
            "           3       0.00      0.00      0.00       228\n",
            "           4       0.33      0.00      0.01       225\n",
            "           5       0.00      0.00      0.00       242\n",
            "           6       0.61      0.05      0.08       242\n",
            "           7       0.11      0.00      0.01       211\n",
            "           8       0.67      0.02      0.03       229\n",
            "           9       0.00      0.00      0.00       240\n",
            "          10       0.18      0.01      0.02       237\n",
            "          11       0.48      0.05      0.09       246\n",
            "          12       0.40      0.02      0.04       210\n",
            "          13       0.00      0.00      0.00       237\n",
            "          14       0.50      0.00      0.01       214\n",
            "          15       0.20      0.00      0.01       224\n",
            "          16       0.35      0.04      0.07       221\n",
            "          17       0.00      0.00      0.00       238\n",
            "          18       0.25      0.00      0.01       215\n",
            "          19       0.07      0.00      0.01       244\n",
            "          20       0.59      0.06      0.12       202\n",
            "          21       0.00      0.00      0.00       242\n",
            "          22       0.00      0.00      0.00       245\n",
            "          23       0.33      0.01      0.01       198\n",
            "          24       0.15      0.01      0.02       227\n",
            "          25       0.20      0.01      0.01       179\n",
            "          26       0.18      0.02      0.04       164\n",
            "          27       0.18      0.01      0.02       241\n",
            "          28       0.00      0.00      0.00       210\n",
            "          29       0.14      0.00      0.01       225\n",
            "          30       0.64      0.27      0.38       147\n",
            "          31       0.12      0.01      0.02       231\n",
            "          32       0.10      0.01      0.02       245\n",
            "          33       0.17      0.00      0.01       246\n",
            "          34       0.00      0.00      0.00       247\n",
            "          35       0.33      0.00      0.01       238\n",
            "          36       0.42      0.06      0.10       241\n",
            "          37       0.00      0.00      0.00       235\n",
            "          38       0.33      0.02      0.04       232\n",
            "          39       0.53      0.14      0.22       241\n",
            "          40       0.00      0.00      0.00       238\n",
            "          41       0.31      0.04      0.07       241\n",
            "          42       0.40      0.01      0.02       246\n",
            "          43       0.00      0.00      0.00       216\n",
            "          44       0.31      0.02      0.04       228\n",
            "          45       0.40      0.02      0.05       243\n",
            "          46       0.00      0.00      0.00       236\n",
            "          47       0.00      0.00      0.00       241\n",
            "          48       0.67      0.01      0.02       247\n",
            "          49       0.25      0.01      0.02       211\n",
            "          50       0.38      0.07      0.12       240\n",
            "          51       0.00      0.00      0.00       243\n",
            "          52       0.00      0.00      0.00       246\n",
            "          53       0.62      0.15      0.24       223\n",
            "          54       0.75      0.01      0.03       234\n",
            "          55       0.50      0.00      0.01       242\n",
            "          56       0.71      0.05      0.09       200\n",
            "          57       0.45      0.02      0.05       211\n",
            "          58       0.00      0.00      0.00       227\n",
            "          59       0.50      0.03      0.06       243\n",
            "          60       0.17      0.00      0.01       241\n",
            "          61       0.00      0.00      0.00       237\n",
            "          62       0.33      0.00      0.01       246\n",
            "          63       0.50      0.00      0.01       244\n",
            "          64       0.40      0.01      0.02       246\n",
            "          65       0.17      0.00      0.01       238\n",
            "          66       0.20      0.03      0.05       248\n",
            "          67       0.00      0.00      0.00       233\n",
            "          68       0.00      0.00      0.00       236\n",
            "          69       0.12      0.01      0.02       247\n",
            "          70       0.59      0.10      0.18       182\n",
            "          71       0.00      0.00      0.00       243\n",
            "          72       0.00      0.00      0.00       240\n",
            "          73       0.33      0.00      0.01       247\n",
            "          74       0.18      0.03      0.05       242\n",
            "          75       0.50      0.01      0.03       145\n",
            "          76       0.71      0.06      0.11       204\n",
            "          77       0.00      0.00      0.00       244\n",
            "          78       0.67      0.02      0.03       238\n",
            "          79       0.00      0.00      0.00       245\n",
            "          80       0.58      0.05      0.09       232\n",
            "          81       0.00      0.00      0.00       233\n",
            "          82       0.67      0.21      0.32       176\n",
            "          83       0.23      0.04      0.06       219\n",
            "          84       0.00      0.00      0.00       236\n",
            "          85       0.40      0.01      0.02       241\n",
            "          86       0.64      0.09      0.16       175\n",
            "          87       0.25      0.01      0.02       244\n",
            "          88       0.12      0.00      0.01       241\n",
            "          89       0.13      0.04      0.06       244\n",
            "          90       0.18      0.01      0.02       238\n",
            "          91       0.00      0.00      0.00       238\n",
            "          92       0.33      0.01      0.02       229\n",
            "          93       0.86      0.03      0.05       221\n",
            "          94       0.57      0.02      0.03       245\n",
            "          95       0.51      1.00      0.67     21262\n",
            "\n",
            "    accuracy                           0.50     43022\n",
            "   macro avg       0.27      0.03      0.04     43022\n",
            "weighted avg       0.38      0.50      0.35     43022\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA 수행"
      ],
      "metadata": {
        "id": "Be0_5B5AEyO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from xgboost import XGBClassifier\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def apply_pca(train_df, test_df, n_components=20):\n",
        "\n",
        "    # df에서 label 분리\n",
        "    X_train = train_df.drop(columns=['label'])\n",
        "    X_test  = test_df.drop(columns=['label'])\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "    pca = PCA(n_components=n_components, random_state=42)\n",
        "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "    X_test_pca  = pca.transform(X_test_scaled)\n",
        "\n",
        "    # PCA된 feature + label 다시 합쳐서 반환\n",
        "    train_pca = train_df[['label']].copy()\n",
        "    test_pca  = test_df[['label']].copy()\n",
        "\n",
        "    train_pca = train_pca.join(\n",
        "        pd.DataFrame(X_train_pca, index=train_df.index)\n",
        "    )\n",
        "    test_pca = test_pca.join(\n",
        "        pd.DataFrame(X_test_pca, index=test_df.index)\n",
        "    )\n",
        "\n",
        "    return train_pca, test_pca\n",
        "\n",
        "\n",
        "train_pca, test_pca = apply_pca(train_df, test_df, n_components=20)\n",
        "train_and_test_with_XGBoost(train_pca, test_pca)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOiq0tRJEzaf",
        "outputId": "f6cc0a34-23af-4486-fe90-7e96916758f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Train shape: (65278, 20)\n",
            "[INFO] Test shape: (21760, 20)\n",
            "\n",
            "================ Closed-World (95-class) ================\n",
            "\n",
            "[INFO] Classes: 95\n",
            "[Closed-World] Accuracy: 0.07619485294117648\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.02      0.01      0.01       239\n",
            "           1       0.03      0.03      0.03       229\n",
            "           2       0.03      0.03      0.03       239\n",
            "           3       0.03      0.01      0.01       228\n",
            "           4       0.05      0.03      0.03       225\n",
            "           5       0.03      0.01      0.02       242\n",
            "           6       0.12      0.11      0.11       242\n",
            "           7       0.05      0.09      0.06       211\n",
            "           8       0.04      0.02      0.03       229\n",
            "           9       0.06      0.03      0.03       240\n",
            "          10       0.05      0.03      0.04       237\n",
            "          11       0.12      0.17      0.14       246\n",
            "          12       0.06      0.18      0.09       210\n",
            "          13       0.03      0.01      0.01       237\n",
            "          14       0.07      0.03      0.04       214\n",
            "          15       0.06      0.08      0.07       224\n",
            "          16       0.09      0.10      0.09       221\n",
            "          17       0.04      0.04      0.04       238\n",
            "          18       0.06      0.07      0.07       215\n",
            "          19       0.04      0.04      0.04       244\n",
            "          20       0.12      0.16      0.14       202\n",
            "          21       0.06      0.07      0.07       242\n",
            "          22       0.05      0.04      0.05       245\n",
            "          23       0.09      0.11      0.10       198\n",
            "          24       0.08      0.11      0.10       227\n",
            "          25       0.09      0.08      0.08       179\n",
            "          26       0.12      0.30      0.17       164\n",
            "          27       0.06      0.05      0.05       241\n",
            "          28       0.07      0.05      0.06       210\n",
            "          29       0.07      0.08      0.07       225\n",
            "          30       0.35      0.39      0.37       147\n",
            "          31       0.08      0.06      0.07       231\n",
            "          32       0.02      0.02      0.02       245\n",
            "          33       0.06      0.09      0.07       246\n",
            "          34       0.03      0.04      0.03       247\n",
            "          35       0.05      0.05      0.05       238\n",
            "          36       0.10      0.08      0.09       241\n",
            "          37       0.01      0.01      0.01       235\n",
            "          38       0.09      0.07      0.08       232\n",
            "          39       0.10      0.08      0.09       241\n",
            "          40       0.04      0.03      0.03       238\n",
            "          41       0.17      0.15      0.16       241\n",
            "          42       0.05      0.03      0.04       246\n",
            "          43       0.10      0.08      0.09       216\n",
            "          44       0.08      0.25      0.12       228\n",
            "          45       0.04      0.02      0.03       243\n",
            "          46       0.06      0.02      0.03       236\n",
            "          47       0.04      0.02      0.02       241\n",
            "          48       0.08      0.08      0.08       247\n",
            "          49       0.07      0.06      0.06       211\n",
            "          50       0.09      0.09      0.09       240\n",
            "          51       0.06      0.03      0.04       243\n",
            "          52       0.03      0.02      0.02       246\n",
            "          53       0.23      0.22      0.22       223\n",
            "          54       0.08      0.03      0.04       234\n",
            "          55       0.06      0.08      0.07       242\n",
            "          56       0.12      0.18      0.14       200\n",
            "          57       0.10      0.12      0.11       211\n",
            "          58       0.05      0.08      0.06       227\n",
            "          59       0.08      0.10      0.09       243\n",
            "          60       0.03      0.02      0.02       241\n",
            "          61       0.08      0.04      0.05       237\n",
            "          62       0.05      0.07      0.06       246\n",
            "          63       0.04      0.02      0.03       244\n",
            "          64       0.05      0.04      0.05       246\n",
            "          65       0.05      0.08      0.06       238\n",
            "          66       0.08      0.17      0.11       248\n",
            "          67       0.05      0.03      0.04       233\n",
            "          68       0.06      0.02      0.03       236\n",
            "          69       0.13      0.08      0.10       247\n",
            "          70       0.09      0.40      0.15       182\n",
            "          71       0.02      0.01      0.02       243\n",
            "          72       0.03      0.01      0.02       240\n",
            "          73       0.06      0.09      0.07       247\n",
            "          74       0.06      0.05      0.05       242\n",
            "          75       0.17      0.43      0.24       145\n",
            "          76       0.16      0.24      0.19       204\n",
            "          77       0.02      0.01      0.01       244\n",
            "          78       0.03      0.03      0.03       238\n",
            "          79       0.04      0.07      0.05       245\n",
            "          80       0.09      0.09      0.09       232\n",
            "          81       0.04      0.03      0.03       233\n",
            "          82       0.47      0.24      0.32       176\n",
            "          83       0.03      0.02      0.03       219\n",
            "          84       0.04      0.08      0.05       236\n",
            "          85       0.03      0.02      0.03       241\n",
            "          86       0.19      0.20      0.19       175\n",
            "          87       0.08      0.05      0.06       244\n",
            "          88       0.04      0.05      0.04       241\n",
            "          89       0.05      0.04      0.04       244\n",
            "          90       0.06      0.07      0.07       238\n",
            "          91       0.08      0.05      0.06       238\n",
            "          92       0.02      0.01      0.01       229\n",
            "          93       0.09      0.15      0.11       221\n",
            "          94       0.06      0.08      0.07       245\n",
            "\n",
            "    accuracy                           0.08     21760\n",
            "   macro avg       0.07      0.08      0.07     21760\n",
            "weighted avg       0.07      0.08      0.07     21760\n",
            "\n",
            "[INFO] Train shape: (129063, 20)\n",
            "[INFO] Test shape: (43022, 20)\n",
            "[INFO] Binary class ratio: [63785 65278]\n",
            "[Binary] Accuracy: 0.5892101715401422\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.48      0.53     21262\n",
            "           1       0.58      0.70      0.63     21760\n",
            "\n",
            "    accuracy                           0.59     43022\n",
            "   macro avg       0.59      0.59      0.58     43022\n",
            "weighted avg       0.59      0.59      0.58     43022\n",
            "\n",
            "\n",
            "================ Open-World Multiclass (0~95) ================\n",
            "\n",
            "[INFO] Classes: 96\n",
            "[Open-Multi] Accuracy: 0.4957696062479662\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       239\n",
            "           1       0.17      0.00      0.01       229\n",
            "           2       0.00      0.00      0.00       239\n",
            "           3       0.00      0.00      0.00       228\n",
            "           4       0.00      0.00      0.00       225\n",
            "           5       0.00      0.00      0.00       242\n",
            "           6       0.33      0.00      0.01       242\n",
            "           7       0.00      0.00      0.00       211\n",
            "           8       0.00      0.00      0.00       229\n",
            "           9       0.00      0.00      0.00       240\n",
            "          10       0.00      0.00      0.00       237\n",
            "          11       0.29      0.01      0.02       246\n",
            "          12       0.00      0.00      0.00       210\n",
            "          13       0.00      0.00      0.00       237\n",
            "          14       0.25      0.00      0.01       214\n",
            "          15       0.50      0.01      0.02       224\n",
            "          16       0.38      0.01      0.03       221\n",
            "          17       0.00      0.00      0.00       238\n",
            "          18       0.00      0.00      0.00       215\n",
            "          19       0.00      0.00      0.00       244\n",
            "          20       0.75      0.01      0.03       202\n",
            "          21       0.00      0.00      0.00       242\n",
            "          22       0.00      0.00      0.00       245\n",
            "          23       0.00      0.00      0.00       198\n",
            "          24       0.00      0.00      0.00       227\n",
            "          25       0.50      0.01      0.01       179\n",
            "          26       0.23      0.02      0.03       164\n",
            "          27       0.20      0.00      0.01       241\n",
            "          28       0.00      0.00      0.00       210\n",
            "          29       0.00      0.00      0.00       225\n",
            "          30       0.74      0.24      0.36       147\n",
            "          31       0.00      0.00      0.00       231\n",
            "          32       0.00      0.00      0.00       245\n",
            "          33       0.00      0.00      0.00       246\n",
            "          34       0.00      0.00      0.00       247\n",
            "          35       0.00      0.00      0.00       238\n",
            "          36       0.50      0.00      0.01       241\n",
            "          37       0.00      0.00      0.00       235\n",
            "          38       0.00      0.00      0.00       232\n",
            "          39       0.00      0.00      0.00       241\n",
            "          40       0.00      0.00      0.00       238\n",
            "          41       0.25      0.00      0.01       241\n",
            "          42       0.00      0.00      0.00       246\n",
            "          43       0.00      0.00      0.00       216\n",
            "          44       0.25      0.01      0.02       228\n",
            "          45       0.00      0.00      0.00       243\n",
            "          46       0.00      0.00      0.00       236\n",
            "          47       0.00      0.00      0.00       241\n",
            "          48       0.00      0.00      0.00       247\n",
            "          49       0.00      0.00      0.00       211\n",
            "          50       0.22      0.02      0.04       240\n",
            "          51       0.00      0.00      0.00       243\n",
            "          52       0.00      0.00      0.00       246\n",
            "          53       0.67      0.07      0.13       223\n",
            "          54       0.00      0.00      0.00       234\n",
            "          55       0.00      0.00      0.00       242\n",
            "          56       0.50      0.01      0.01       200\n",
            "          57       0.33      0.00      0.01       211\n",
            "          58       0.00      0.00      0.00       227\n",
            "          59       0.50      0.01      0.02       243\n",
            "          60       0.00      0.00      0.00       241\n",
            "          61       0.00      0.00      0.00       237\n",
            "          62       1.00      0.00      0.01       246\n",
            "          63       0.00      0.00      0.00       244\n",
            "          64       0.00      0.00      0.00       246\n",
            "          65       0.00      0.00      0.00       238\n",
            "          66       1.00      0.01      0.02       248\n",
            "          67       0.00      0.00      0.00       233\n",
            "          68       0.00      0.00      0.00       236\n",
            "          69       0.29      0.02      0.03       247\n",
            "          70       0.33      0.03      0.05       182\n",
            "          71       0.00      0.00      0.00       243\n",
            "          72       0.00      0.00      0.00       240\n",
            "          73       0.00      0.00      0.00       247\n",
            "          74       0.00      0.00      0.00       242\n",
            "          75       0.00      0.00      0.00       145\n",
            "          76       0.43      0.03      0.06       204\n",
            "          77       0.00      0.00      0.00       244\n",
            "          78       0.00      0.00      0.00       238\n",
            "          79       0.00      0.00      0.00       245\n",
            "          80       0.00      0.00      0.00       232\n",
            "          81       0.00      0.00      0.00       233\n",
            "          82       0.66      0.15      0.25       176\n",
            "          83       0.08      0.00      0.01       219\n",
            "          84       0.00      0.00      0.00       236\n",
            "          85       0.00      0.00      0.00       241\n",
            "          86       0.56      0.03      0.05       175\n",
            "          87       0.17      0.00      0.01       244\n",
            "          88       0.00      0.00      0.00       241\n",
            "          89       0.07      0.00      0.01       244\n",
            "          90       0.00      0.00      0.00       238\n",
            "          91       0.00      0.00      0.00       238\n",
            "          92       0.00      0.00      0.00       229\n",
            "          93       0.00      0.00      0.00       221\n",
            "          94       0.00      0.00      0.00       245\n",
            "          95       0.50      1.00      0.66     21262\n",
            "\n",
            "    accuracy                           0.50     43022\n",
            "   macro avg       0.13      0.02      0.02     43022\n",
            "weighted avg       0.31      0.50      0.33     43022\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}