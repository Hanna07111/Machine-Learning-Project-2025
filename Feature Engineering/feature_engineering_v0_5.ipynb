{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# feature_engineering_v0.5.py\n",
        "\n",
        "# 1. 개요\n",
        "`feature_engineering_v0.5.py` 파일은 Raw Data(Pickle) 를 입력으로 받아  \n",
        "피처를 추출하고, 피처 세트별 성능을 검증한 뒤 최종 학습용 CSV 데이터셋을 생성하는 전처리 파일입니다.\n",
        "\n",
        "---\n",
        "\n",
        "# 2. 주요 기능 -  Feature Engineering: Extraction & Validation\n",
        "**설명:**  \n",
        "- Raw Data로부터 **Non-leaky Feature** 및 **Fingerprint Feature**를 추출  \n",
        "- 피처 세트 **Set A(Only Non-leaky)**, **Set B(Only Fingerprint)**, **Set A+B(Non-leaky + Fingerprint)** 의 모델 성능을 비교 검증  \n",
        "- 검증 결과를 기반으로 최종 학습용 CSV 파일 생성\n",
        "\n"
      ],
      "metadata": {
        "id": "4Vd9QW0iBzBN"
      },
      "id": "4Vd9QW0iBzBN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a908dce5",
      "metadata": {
        "id": "a908dce5",
        "outputId": "d9f879b9-efff-47fe-fda4-eb73834dfd16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- STAGE 1: Extracting, Splitting, and Saving CSVs ---\n",
            "Loading mon_standard.pkl...\n",
            "Loading unmon_standard10.pkl...\n",
            "Processing Monitored data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 950/950 [00:48<00:00, 19.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Unmonitored data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:29<00:00, 334.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing and splitting Dataset A...\n",
            "✓ Saved train_set_A.csv (Shape: (20300, 19))\n",
            "✓ Saved test_set_A.csv (Shape: (8700, 19))\n",
            "\n",
            "Processing and splitting Dataset B...\n",
            "✓ Saved train_set_B.csv (Shape: (20300, 9))\n",
            "✓ Saved test_set_B.csv (Shape: (8700, 9))\n",
            "\n",
            "Processing and splitting Dataset AB...\n",
            "✓ Saved train_set_AB.csv (Shape: (20300, 27))\n",
            "✓ Saved test_set_AB.csv (Shape: (8700, 27))\n",
            "\n",
            "--- STAGE 2: Loading from CSV and Training Models ---\n",
            "\n",
            "================================================================================\n",
            "      TESTING: Set A (Unleaky Only)\n",
            "================================================================================\n",
            "Features: 18 | Train samples: 20300 | Test samples: 8700\n",
            "Training Single Multi-Class XGBoost...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Hwang Jieun\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [15:52:30] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating...\n",
            "\n",
            "================================================================================\n",
            "      TESTING: Set B (Fingerprint Only)\n",
            "================================================================================\n",
            "Features: 8 | Train samples: 20300 | Test samples: 8700\n",
            "Training Single Multi-Class XGBoost...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Hwang Jieun\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [15:54:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating...\n",
            "\n",
            "================================================================================\n",
            "      TESTING: Set A+B (All Features)\n",
            "================================================================================\n",
            "Features: 26 | Train samples: 20300 | Test samples: 8700\n",
            "Training Single Multi-Class XGBoost...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Hwang Jieun\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [15:57:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating...\n",
            "\n",
            "================================================================================\n",
            "           FINAL FEATURE SET COMPARISON (Single Multi-Class Model)\n",
            "================================================================================\n",
            "                   Model  Features  Accuracy  Macro F1  Macro ROC-AUC  Unmon (95) Recall\n",
            "    Set A (Unleaky Only)        18    0.5201    0.3954         0.9512             0.7323\n",
            "Set B (Fingerprint Only)         8    0.5643    0.5574         0.9658             0.4807\n",
            "  Set A+B (All Features)        26    0.7937    0.7575         0.9913             0.8323\n",
            "\n",
            "================================================================================\n",
            "      'Set A+B' 모델의 binary classification 성능 (Mon vs Unmon)\n",
            "================================================================================\n",
            "Total Binary Accuracy: 0.9109\n",
            "\n",
            " Binary Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "Unmonitored (0)       0.90      0.83      0.87      3000\n",
            "  Monitored (1)       0.92      0.95      0.93      5700\n",
            "\n",
            "       accuracy                           0.91      8700\n",
            "      macro avg       0.91      0.89      0.90      8700\n",
            "   weighted avg       0.91      0.91      0.91      8700\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "import os\n",
        "\n",
        "# non leaky Feature 추출 (Set A: 18개)\n",
        "def extract_non_leaky_features(trace, N_PACKETS=100, T_SECONDS=5.0, N_PACKETS_30=30):\n",
        "    if trace is None or len(trace) == 0: return {}\n",
        "    timestamps = np.abs(np.array(trace))\n",
        "    directions = np.sign(np.array(trace))\n",
        "    if directions[0] == 0: directions[0] = 1\n",
        "    trace_len = len(timestamps)\n",
        "    features = {}\n",
        "\n",
        "    n = min(trace_len, N_PACKETS)\n",
        "    ts_n = timestamps[:n]\n",
        "    dir_n = directions[:n]\n",
        "    features['time_for_N_pkts'] = ts_n[-1]\n",
        "    features['out_ratio_N_pkts'] = np.sum(dir_n == 1) / n\n",
        "    features['in_ratio_N_pkts'] = np.sum(dir_n == -1) / n\n",
        "    if n > 1:\n",
        "        ipt_n = np.diff(ts_n)\n",
        "        features['avg_ipt_N_pkts'] = np.mean(ipt_n)\n",
        "        features['std_ipt_N_pkts'] = np.std(ipt_n)\n",
        "    else:\n",
        "        features['avg_ipt_N_pkts'] = 0\n",
        "        features['std_ipt_N_pkts'] = 0\n",
        "\n",
        "    mask_t = timestamps <= T_SECONDS\n",
        "    ts_t = timestamps[mask_t]s\n",
        "    dir_t = directions[mask_t]\n",
        "    n_t = len(ts_t)\n",
        "    features['packets_in_T_sec'] = n_t\n",
        "    if n_t > 0:\n",
        "        features['out_ratio_T_sec'] = np.sum(dir_t == 1) / n_t\n",
        "        features['in_ratio_T_sec'] = np.sum(dir_t == -1) / n_t\n",
        "        if n_t > 1:\n",
        "            ipt_t = np.diff(ts_t)\n",
        "            features['avg_ipt_T_sec'] = np.mean(ipt_t)\n",
        "            features['std_ipt_T_sec'] = np.std(ipt_t)\n",
        "        else:\n",
        "            features['avg_ipt_T_sec'] = 0\n",
        "            features['std_ipt_T_sec'] = 0\n",
        "    else:\n",
        "        features['out_ratio_T_sec'] = 0\n",
        "        features['in_ratio_T_sec'] = 0\n",
        "        features['avg_ipt_T_sec'] = 0\n",
        "        features['std_ipt_T_sec'] = 0\n",
        "\n",
        "    first_in_idx = np.where(directions == -1)[0]\n",
        "    if len(first_in_idx) > 0:\n",
        "        first_in_idx = first_in_idx[0]\n",
        "        features['time_to_first_in'] = timestamps[first_in_idx]\n",
        "        features['pkts_before_first_in'] = first_in_idx\n",
        "    else:\n",
        "        features['time_to_first_in'] = -1\n",
        "        features['pkts_before_first_in'] = trace_len\n",
        "\n",
        "    if n > 1:\n",
        "        direction_changes = np.diff(dir_n) != 0\n",
        "        features['burst_count_N_pkts'] = np.sum(direction_changes)\n",
        "        if features['burst_count_N_pkts'] > 0:\n",
        "            burst_lengths = np.diff(np.where(np.concatenate(([True], direction_changes, [True])))[0])\n",
        "            features['avg_burst_len_N_pkts'] = np.mean(burst_lengths)\n",
        "        else:\n",
        "            features['avg_burst_len_N_pkts'] = n\n",
        "    else:\n",
        "        features['burst_count_N_pkts'] = 0\n",
        "        features['avg_burst_len_N_pkts'] = n\n",
        "\n",
        "    ts_out_n = ts_n[dir_n == 1]\n",
        "    ts_in_n = ts_n[dir_n == -1]\n",
        "    features['avg_ipt_out_N_pkts'] = np.mean(np.diff(ts_out_n)) if len(ts_out_n) > 1 else 0\n",
        "    features['avg_ipt_in_N_pkts'] = np.mean(np.diff(ts_in_n)) if len(ts_in_n) > 1 else 0\n",
        "\n",
        "    n_30 = min(trace_len, N_PACKETS_30)\n",
        "    dir_n_30 = directions[:n_30]\n",
        "    features['in_ratio_N_pkts_30'] = np.sum(dir_n_30 == -1) / n_30 if n_30 > 0 else 0\n",
        "    features['out_ratio_N_pkts_30'] = np.sum(dir_n_30 == 1) / n_30 if n_30 > 0 else 0\n",
        "    return features\n",
        "\n",
        "# fingerprint feature 추출 (Set B: 8개)\n",
        "def extract_fingerprint_features(trace):\n",
        "    if trace is None or len(trace) == 0: return {}\n",
        "    timestamps = np.abs(np.array(trace))\n",
        "    directions = np.sign(np.array(trace))\n",
        "    if directions[0] == 0: directions[0] = 1\n",
        "\n",
        "    features = {}\n",
        "    features['total_packets'] = len(timestamps)\n",
        "    features['t_max'] = timestamps[-1] if len(timestamps) > 0 else 0\n",
        "    features['t_mean'] = np.mean(timestamps)\n",
        "    features['t_std'] = np.std(timestamps)\n",
        "    features['num_in'] = np.sum(directions == -1)\n",
        "    features['num_out'] = np.sum(directions == 1)\n",
        "    if features['total_packets'] > 0:\n",
        "        features['in_ratio_total'] = features['num_in'] / features['total_packets']\n",
        "        features['out_ratio_total'] = features['num_out'] / features['total_packets']\n",
        "    else:\n",
        "        features['in_ratio_total'] = 0\n",
        "        features['out_ratio_total'] = 0\n",
        "    return features\n",
        "\n",
        "# All feature 추출 (Set A+B: 26개) ---\n",
        "def extract_all_features(trace):\n",
        "    if trace is None or len(trace) == 0: return {}\n",
        "    features = extract_fingerprint_features(trace)\n",
        "    non_leaky_feats = extract_non_leaky_features(trace)\n",
        "    features.update(non_leaky_feats)\n",
        "    return features\n",
        "\n",
        "def load_pickle(filepath):\n",
        "    print(f\"Loading {filepath}...\")\n",
        "    try:\n",
        "        with open(filepath, 'rb') as f:\n",
        "            return pickle.load(f, encoding='latin1')\n",
        "    except Exception as e:\n",
        "        print(f\"!!! Error loading {filepath}: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_data_for_comparison(mon_data, unmon_data):\n",
        "    data_A, data_B, data_AB = [], [], []\n",
        "\n",
        "    print(\"Processing Monitored data...\")\n",
        "    for site_id, traces in tqdm(mon_data.items()):\n",
        "        for trace in traces:\n",
        "            label = site_id // 10  # 0-94 레이블\n",
        "            data_A.append({**extract_non_leaky_features(trace), 'label': label})\n",
        "            data_B.append({**extract_fingerprint_features(trace), 'label': label})\n",
        "            data_AB.append({**extract_all_features(trace), 'label': label})\n",
        "\n",
        "    print(\"Processing Unmonitored data...\")\n",
        "    for trace in tqdm(unmon_data):\n",
        "        label = 95 # Unmonitored = 95\n",
        "        data_A.append({**extract_non_leaky_features(trace), 'label': label})\n",
        "        data_B.append({**extract_fingerprint_features(trace), 'label': label})\n",
        "        data_AB.append({**extract_all_features(trace), 'label': label})\n",
        "\n",
        "    df_A = pd.DataFrame(data_A).fillna(0).replace([np.inf, -np.inf], 0)\n",
        "    df_B = pd.DataFrame(data_B).fillna(0).replace([np.inf, -np.inf], 0)\n",
        "    df_AB = pd.DataFrame(data_AB).fillna(0).replace([np.inf, -np.inf], 0)\n",
        "\n",
        "    return df_A, df_B, df_AB\n",
        "\n",
        "def train_evaluate_model(train_csv_path, test_csv_path, model_name):\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"      TESTING: {model_name}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    try:\n",
        "        train_df = pd.read_csv(train_csv_path)\n",
        "        test_df = pd.read_csv(test_csv_path)\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"!!! Error loading CSV: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "    X_train = train_df.drop('label', axis=1)\n",
        "    y_train = train_df['label'].astype(int)\n",
        "    X_test = test_df.drop('label', axis=1)\n",
        "    y_test = test_df['label'].astype(int)\n",
        "\n",
        "    num_classes = 96 # 0-95\n",
        "    sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
        "\n",
        "    print(f\"Features: {X_train.shape[1]} | Train samples: {X_train.shape[0]} | Test samples: {X_test.shape[0]}\")\n",
        "    print(\"Training Single Multi-Class XGBoost...\")\n",
        "\n",
        "    model = XGBClassifier(\n",
        "        objective='multi:softmax',\n",
        "        num_class=num_classes,\n",
        "        n_estimators=500,\n",
        "        max_depth=8,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42,\n",
        "        eval_metric='mlogloss',\n",
        "        use_label_encoder=False\n",
        "    )\n",
        "    model.fit(X_train, y_train, sample_weight=sample_weights)\n",
        "\n",
        "    print(\"Evaluating...\")\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    auc = roc_auc_score(y_test, y_proba, multi_class='ovr', average='macro')\n",
        "\n",
        "    report = classification_report(y_test, y_pred, labels=[95], output_dict=True, zero_division=0)\n",
        "    recall_95 = report['95']['recall']\n",
        "\n",
        "    metrics = {\n",
        "        \"Model\": model_name,\n",
        "        \"Features\": X_train.shape[1],\n",
        "        \"Accuracy\": acc,\n",
        "        \"Macro F1\": f1,\n",
        "        \"Macro ROC-AUC\": auc,\n",
        "        \"Unmon (95) Recall\": recall_95\n",
        "    }\n",
        "\n",
        "    return metrics, y_test, y_pred\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    MON_FILE = 'mon_standard.pkl'\n",
        "    UNMON_FILE = 'unmon_standard10.pkl'\n",
        "\n",
        "    #  pkl -> feature 추출 -> train/test 분할 -> 6개 CSV 저장\n",
        "    print(\"--- STAGE 1: Extracting, Splitting, and Saving CSVs ---\")\n",
        "\n",
        "    mon_data_main = load_pickle(MON_FILE)\n",
        "    unmon_data_main = load_pickle(UNMON_FILE)\n",
        "\n",
        "    if mon_data_main is not None and unmon_data_main is not None:\n",
        "\n",
        "        df_A, df_B, df_AB = process_data_for_comparison(mon_data_main, unmon_data_main)\n",
        "\n",
        "        paths = {\n",
        "            'A': ('train_set_A.csv', 'test_set_A.csv'),\n",
        "            'B': ('train_set_B.csv', 'test_set_B.csv'),\n",
        "            'AB': ('train_set_AB.csv', 'test_set_AB.csv')\n",
        "        }\n",
        "\n",
        "        for key, df in [('A', df_A), ('B', df_B), ('AB', df_AB)]:\n",
        "            train_path, test_path = paths[key]\n",
        "\n",
        "            print(f\"\\nProcessing and splitting Dataset {key}...\")\n",
        "            X = df.drop('label', axis=1)\n",
        "            y = df['label']\n",
        "\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X, y, test_size=0.3, random_state=42, stratify=y\n",
        "            )\n",
        "\n",
        "            train_df = pd.concat([X_train.reset_index(drop=True),\n",
        "                                  y_train.reset_index(drop=True)], axis=1)\n",
        "            test_df = pd.concat([X_test.reset_index(drop=True),\n",
        "                                 y_test.reset_index(drop=True)], axis=1)\n",
        "\n",
        "            train_df.to_csv(train_path, index=False)\n",
        "            test_df.to_csv(test_path, index=False)\n",
        "            print(f\"✓ Saved {train_path} (Shape: {train_df.shape})\")\n",
        "            print(f\"✓ Saved {test_path} (Shape: {test_df.shape})\")\n",
        "\n",
        "    else:\n",
        "        print(\"--- Data loading failed. Aborting script. ---\")\n",
        "        exit()\n",
        "\n",
        "    print(\"\\n--- STAGE 2: Loading from CSV and Training Models ---\")\n",
        "\n",
        "    results_list = []\n",
        "\n",
        "    # 모델 1 (Unleaky Only)\n",
        "    results_A, _, _ = train_evaluate_model(paths['A'][0], paths['A'][1], \"Set A (Unleaky Only)\")\n",
        "    if results_A: results_list.append(results_A)\n",
        "\n",
        "    # 모델 2 (Fingerprint Only)\n",
        "    results_B, _, _ = train_evaluate_model(paths['B'][0], paths['B'][1], \"Set B (Fingerprint Only)\")\n",
        "    if results_B: results_list.append(results_B)\n",
        "\n",
        "    # 모델 3 (All Features)\n",
        "    results_AB, y_test_AB, y_pred_AB = train_evaluate_model(paths['AB'][0], paths['AB'][1], \"Set A+B (All Features)\")\n",
        "    if results_AB: results_list.append(results_AB)\n",
        "\n",
        "    # 최종 리포트\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"           FINAL FEATURE SET COMPARISON (Single Multi-Class Model)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    results_df = pd.DataFrame(results_list)\n",
        "    print(results_df.to_string(index=False, float_format=\"%.4f\"))\n",
        "\n",
        "    # Set A+B 모델의 이진 분류 성능 분석\n",
        "    if 'y_test_AB' in locals():\n",
        "        y_test_binary = np.where(y_test_AB == 95, 0, 1)\n",
        "        y_pred_binary = np.where(y_pred_AB == 95, 0, 1)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"      'Set A+B' 모델의 binary classification 성능 (Mon vs Unmon)\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        print(f\"Total Binary Accuracy: {accuracy_score(y_test_binary, y_pred_binary):.4f}\")\n",
        "        print(\"\\n Binary Classification Report:\")\n",
        "        print(classification_report(y_test_binary, y_pred_binary, target_names=['Unmonitored (0)', 'Monitored (1)']))\n",
        "    else:\n",
        "        print(\"\\n--- Skipping Binary Analysis (Model A+B failed) ---\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}